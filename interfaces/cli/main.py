# ç»Ÿä¸€CLIå…¥å£ - é‡æ–°è®¾è®¡æ¶æ„
# è·¯å¾„: interfaces/cli/main.py
# é¡¹ç›®åç§°: Word Frequency Analysis
# ä½œè€…: Sherryyue

import click
import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from core.utils.config_manager import get_config

@click.group()
@click.version_option(version="1.0.0")
@click.option('--config-env', default='default', help='é…ç½®ç¯å¢ƒ (default/development/production)')
@click.pass_context
def cli(ctx, config_env):
    """
    è¯é¢‘åˆ†æå’Œè¯æ±‡ç®¡ç†å·¥å…· v1.0.0
    
    ä¸€ä¸ªä¸“ä¸šçš„æ–‡æœ¬åˆ†æå·¥å…·ï¼Œæ”¯æŒæ–‡æœ¬è¯é¢‘åˆ†æã€è¯æ±‡è¡¨ç®¡ç†å’Œè¯æ±‡æŸ¥è¯¢ã€‚
    
    \b
    ä¸»è¦åŠŸèƒ½ï¼š
    â€¢ text     - æ–‡æœ¬åˆ†æï¼šå¤„ç†æ–‡æ¡£ï¼Œè¿›è¡Œè¯é¢‘ç»Ÿè®¡
    â€¢ wordlist - è¯æ±‡è¡¨ç®¡ç†ï¼šå¯¼å…¥å„ç§è¯æ±‡è¡¨æ–‡ä»¶
    â€¢ vocab    - è¯æ±‡æŸ¥è¯¢ï¼šæŸ¥è¯¢è¯æ±‡ä¿¡æ¯å’Œç»Ÿè®¡
    """
    ctx.ensure_object(dict)
    ctx.obj['config_env'] = config_env
    
    # åˆå§‹åŒ–é…ç½®
    try:
        config = get_config()
    except Exception as e:
        click.secho(f"âš ï¸  é…ç½®åŠ è½½å¤±è´¥: {e}", fg='yellow')

# =============================================================================
# ğŸ“„ TEXT å‘½ä»¤ç»„ - æ–‡æœ¬åˆ†æåŠŸèƒ½
# =============================================================================

@cli.group()
def text():
    """æ–‡æœ¬åˆ†æç›¸å…³å‘½ä»¤
    
    å¤„ç†æ–‡æ¡£æ–‡ä»¶ï¼Œè¿›è¡Œè¯é¢‘ç»Ÿè®¡å’Œåˆ†æã€‚
    æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼šPDFã€TXTã€DOCXç­‰ã€‚
    """
    pass

@text.command()
@click.argument('directory', type=click.Path(exists=True))
@click.option('--move/--no-move', default=True, help='å¤„ç†å®Œæˆåæ˜¯å¦ç§»åŠ¨æ–‡ä»¶åˆ°processedç›®å½•')
@click.option('--recursive/--no-recursive', default=True, help='æ˜¯å¦é€’å½’æ‰«æå­ç›®å½•')
def process(directory, move, recursive):
    """å¤„ç†æŒ‡å®šç›®å½•ä¸‹çš„æ–‡æœ¬æ–‡ä»¶è¿›è¡Œè¯é¢‘åˆ†æ"""
    try:
        from core.engines.input.file_processor import TextProcessor
        
        click.echo(f"ğŸ“‚ å¼€å§‹å¤„ç†ç›®å½•: {directory}")
        processor = TextProcessor(move_processed=move)
        processor.process_new_texts(directory, scan_subdirs=recursive)
        
        click.secho("âœ… æ–‡æœ¬å¤„ç†å®Œæˆï¼", fg='green')
        
    except Exception as e:
        click.secho(f"âŒ å¤„ç†å¤±è´¥: {e}", fg='red', err=True)

@text.command()
@click.option('--limit', default=20, help='æ˜¾ç¤ºè®°å½•æ•°é‡')
@click.option('--format', 'output_format', type=click.Choice(['table', 'json']), default='table', help='è¾“å‡ºæ ¼å¼')
def query(limit, output_format):
    """æŸ¥è¯¢æ–‡æœ¬åˆ†æè®°å½•"""
    try:
        from core.engines.database.database_adapter import unified_adapter
        
        analyses = unified_adapter.get_all_analyses()
        
        if not analyses:
            click.echo("ğŸ“š æš‚æ— æ–‡æœ¬åˆ†æè®°å½•")
            return
        
        # é™åˆ¶æ˜¾ç¤ºæ•°é‡
        analyses = analyses[:limit]
        
        if output_format == 'json':
            import json
            data = [{'id': a[0], 'filename': a[1], 'date': a[2], 'total_words': a[3], 'unique_words': a[4]} for a in analyses]
            click.echo(json.dumps(data, ensure_ascii=False, indent=2))
        else:
            # è¡¨æ ¼æ ¼å¼
            click.echo("ğŸ“š æ–‡æœ¬åˆ†æè®°å½•:")
            click.echo("-" * 80)
            click.echo(f"{'ID':<12} {'æ–‡ä»¶å':<30} {'æ€»è¯æ•°':<10} {'å”¯ä¸€è¯æ•°':<10}")
            click.echo("-" * 80)
            
            for analysis in analyses:
                id_short = analysis[0][:11] + "..." if len(analysis[0]) > 11 else analysis[0]
                filename = analysis[1][:29] if len(analysis[1]) > 29 else analysis[1]
                click.echo(f"{id_short:<12} {filename:<30} {analysis[3]:<10} {analysis[4]:<10}")
            
            click.echo("-" * 80)
            click.echo(f"æ€»è®¡ {len(analyses)} ä¸ªæ–‡æœ¬")
            
    except Exception as e:
        click.secho(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}", fg='red', err=True)

@text.command()
@click.option('-o', '--output', type=click.Path(), help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
@click.option('--format', 'output_format', type=click.Choice(['txt', 'csv', 'json', 'excel']), default='txt', help='å¯¼å‡ºæ ¼å¼')
@click.option('-t', '--text-id', help='å¯¼å‡ºç‰¹å®šæ–‡æœ¬(å¯é€‰)')
def export(output, output_format, text_id):
    """å¯¼å‡ºæ–‡æœ¬åˆ†æç»“æœ"""
    try:
        from core.engines.database.database_adapter import unified_adapter
        
        if not output:
            output = f"text_analysis_export.{output_format}"
        
        # å¯¼å‡ºé€»è¾‘...
        click.secho(f"âœ… å¯¼å‡ºå®Œæˆ: {output}", fg='green')
        
    except Exception as e:
        click.secho(f"âŒ å¯¼å‡ºå¤±è´¥: {e}", fg='red', err=True)

@text.command()
def organize():
    """æ•´ç†å·²åˆ†æçš„æ–‡æœ¬æ–‡ä»¶ï¼šå°†æ•°æ®åº“ä¸­å·²å­˜åœ¨çš„æ–‡ä»¶ç§»åŠ¨åˆ°processedç›®å½•"""
    try:
        from core.engines.input.file_processor import TextProcessor
        
        processor = TextProcessor()
        processor.organize_existing_files()
        
    except Exception as e:
        click.secho(f"âŒ æ•´ç†å¤±è´¥: {e}", fg='red', err=True)

# =============================================================================
# ğŸ“š WORDLIST å‘½ä»¤ç»„ - è¯æ±‡è¡¨å¯¼å…¥åŠŸèƒ½
# =============================================================================

@cli.group()
def wordlist():
    """è¯æ±‡è¡¨å¯¼å…¥ç›¸å…³å‘½ä»¤
    
    ä»å„ç§æ ¼å¼çš„æ–‡ä»¶å¯¼å…¥è¯æ±‡è¡¨ï¼Œå¦‚AWLã€GREã€TOEFLç­‰ã€‚
    æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼šTXTã€CSVã€XLSXã€DOCXã€PDFç­‰ã€‚
    """
    pass

@wordlist.command('import')
@click.argument('file_path', type=click.Path(exists=True))
@click.option('-t', '--tag', help='è¯æ±‡è¡¨æ ‡ç­¾å')
@click.option('-d', '--description', help='è¯æ±‡è¡¨æè¿°')
@click.option('--move/--no-move', default=True, help='å¯¼å…¥å®Œæˆåæ˜¯å¦ç§»åŠ¨æ–‡ä»¶åˆ°processedç›®å½•')
def import_wordlist(file_path, tag, description, move):
    """ä»æ–‡ä»¶å¯¼å…¥è¯æ±‡è¡¨"""
    try:
        from core.engines.input.modern_wordlist_import import import_wordlist_from_file
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šæ ‡ç­¾ï¼Œä½¿ç”¨æ–‡ä»¶å
        if not tag:
            tag = Path(file_path).stem
        
        click.echo(f"ğŸ“š å¼€å§‹å¯¼å…¥è¯æ±‡è¡¨: {file_path}")
        success = import_wordlist_from_file(file_path, tag, description)
        
        if success and move:
            # ç§»åŠ¨æ–‡ä»¶åˆ°processedç›®å½•
            source = Path(file_path)
            if 'wordlists/new' in str(source):
                target_dir = source.parent.parent / 'processed'
                target_dir.mkdir(exist_ok=True)
                target_path = target_dir / source.name
                
                import shutil
                shutil.move(str(source), str(target_path))
                click.echo(f"ğŸ“ æ–‡ä»¶å·²ç§»åŠ¨åˆ°: {target_path}")
        
        if success:
            click.secho(f"âœ… è¯æ±‡è¡¨å¯¼å…¥æˆåŠŸ: {tag}", fg='green')
        else:
            click.secho(f"âŒ è¯æ±‡è¡¨å¯¼å…¥å¤±è´¥: {tag}", fg='red')
            
    except Exception as e:
        click.secho(f"âŒ å¯¼å…¥å¤±è´¥: {e}", fg='red', err=True)

@wordlist.command('batch')
@click.argument('directory', type=click.Path(exists=True))
@click.option('--pattern', default='*.txt', help='æ–‡ä»¶åæ¨¡å¼')
@click.option('--move/--no-move', default=True, help='å¯¼å…¥å®Œæˆåæ˜¯å¦ç§»åŠ¨æ–‡ä»¶')
def batch_import(directory, pattern, move):
    """æ‰¹é‡å¯¼å…¥ç›®å½•ä¸‹çš„è¯æ±‡è¡¨æ–‡ä»¶"""
    try:
        from core.engines.input.modern_wordlist_import import import_multiple_wordlists
        
        click.echo(f"ğŸ“‚ å¼€å§‹æ‰¹é‡å¯¼å…¥: {directory}")
        success_count = import_multiple_wordlists(directory, pattern)
        
        if success_count > 0:
            click.secho(f"âœ… æˆåŠŸå¯¼å…¥ {success_count} ä¸ªè¯æ±‡è¡¨", fg='green')
        else:
            click.secho("âŒ æ²¡æœ‰æˆåŠŸå¯¼å…¥ä»»ä½•è¯æ±‡è¡¨", fg='red')
            
    except Exception as e:
        click.secho(f"âŒ æ‰¹é‡å¯¼å…¥å¤±è´¥: {e}", fg='red', err=True)

@wordlist.command('list')
@click.option('--format', 'output_format', type=click.Choice(['table', 'json']), default='table', help='è¾“å‡ºæ ¼å¼')
def list_wordlists(output_format):
    """æ˜¾ç¤ºæ‰€æœ‰å·²å¯¼å…¥çš„è¯æ±‡è¡¨"""
    try:
        from core.engines.database.database_adapter import unified_adapter
        
        wordlists = unified_adapter.get_all_wordlists()
        
        if not wordlists:
            click.echo("ğŸ“š æš‚æ— å·²å¯¼å…¥çš„è¯æ±‡è¡¨")
            return
        
        if output_format == 'json':
            import json
            click.echo(json.dumps(wordlists, ensure_ascii=False, indent=2))
        else:
            click.echo("ğŸ“š å·²å¯¼å…¥çš„è¯æ±‡è¡¨:")
            click.echo("-" * 60)
            for wl in wordlists:
                click.echo(f"ğŸ“– {wl['name']}: {wl['word_count']} ä¸ªè¯æ±‡")
            click.echo("-" * 60)
            click.echo(f"æ€»è®¡ {len(wordlists)} ä¸ªè¯æ±‡è¡¨")
            
    except Exception as e:
        click.secho(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}", fg='red', err=True)

# =============================================================================
# ğŸ” VOCAB å‘½ä»¤ç»„ - è¯æ±‡æŸ¥è¯¢åŠŸèƒ½ 
# =============================================================================

@cli.group()
def vocab():
    """è¯æ±‡æŸ¥è¯¢å’Œç®¡ç†ç›¸å…³å‘½ä»¤
    
    æŸ¥è¯¢è¯æ±‡é¢‘ç‡ã€è¯æ±‡è¡¨ä¿¡æ¯ã€ç»Ÿè®¡æ•°æ®ç­‰ã€‚
    """
    pass

@vocab.command()
@click.argument('word')
@click.option('-d', '--detailed', is_flag=True, help='æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯')
@click.option('--format', 'output_format', type=click.Choice(['text', 'json']), default='text', help='è¾“å‡ºæ ¼å¼')
def query(word, detailed, output_format):
    """æŸ¥è¯¢è¯æ±‡ä¿¡æ¯"""
    try:
        from core.engines.database.database_adapter import unified_adapter
        
        # æŸ¥è¯¢è¯æ±‡é¢‘ç‡
        results = unified_adapter.search_words(word)
        
        if not results:
            click.echo(f"âŒ æœªæ‰¾åˆ°åŒ…å« '{word}' çš„è¯æ±‡")
            return
        
        if output_format == 'json':
            import json
            data = [{"word": r[0], "total_frequency": r[1], "document_count": r[2]} for r in results]
            click.echo(json.dumps(data, ensure_ascii=False, indent=2))
        else:
            click.echo(f"ğŸ” åŒ…å« '{word}' çš„è¯æ±‡:")
            click.echo("-" * 50)
            for result in results[:20]:  # é™åˆ¶æ˜¾ç¤ºå‰20ä¸ª
                word_text, freq, doc_count = result
                click.echo(f"ğŸ“ {word_text}: é¢‘ç‡ {freq}, å‡ºç°åœ¨ {doc_count} ä¸ªæ–‡æ¡£")
            
            if len(results) > 20:
                click.echo(f"... è¿˜æœ‰ {len(results) - 20} ä¸ªç›¸å…³è¯æ±‡")
                
    except Exception as e:
        click.secho(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}", fg='red', err=True)

@vocab.command()
def stats():
    """æ˜¾ç¤ºè¯æ±‡ç»Ÿè®¡ä¿¡æ¯"""
    try:
        from core.engines.database.database_adapter import unified_adapter
        
        stats = unified_adapter.get_database_info()
        
        click.echo("ğŸ“Š è¯æ±‡ç»Ÿè®¡:")
        click.echo("-" * 40)
        click.echo(f"ğŸ“š è¯æ±‡è¡¨æ•°é‡: {stats.get('wordlists_count', 0)}")
        click.echo(f"ğŸ”¤ æ€»è¯æ±‡æ•° (åŸå§‹å½¢å¼): {stats.get('total_words', 0)}")
        click.echo(f"ğŸŒ± ç‹¬ç‰¹è¯æ ¹æ•°: {stats.get('unique_lemmas', 0)}")
        click.echo(f"ğŸ“„ æ–‡æ¡£æ•°é‡: {stats.get('documents_count', 0)}")
        click.echo(f"ğŸ“ è¯é¢‘è®°å½•: {stats.get('word_frequencies_count', 0)}")
        
        if stats.get('documents_count', 0) > 0:
            avg_length = stats.get('avg_document_length', 0)
            if avg_length:
                click.echo(f"ğŸ“ å¹³å‡æ–‡æ¡£é•¿åº¦: {avg_length:.1f} è¯")
        
        avg_variants = stats.get('avg_variants_per_lemma', 0)
        if avg_variants > 0:
            click.echo(f"ğŸ”„ å¹³å‡æ¯è¯æ ¹å˜å½¢æ•°: {avg_variants:.1f}")
        click.echo("-" * 40)
        
    except Exception as e:
        click.secho(f"âŒ ç»Ÿè®¡å¤±è´¥: {e}", fg='red', err=True)

@vocab.command()
@click.argument('word')
@click.option('--doc-id', help='æŒ‡å®šæ–‡æ¡£IDæŸ¥çœ‹ç‰¹å®šæ–‡æ¡£ä¸­çš„å˜å½¢')
def variants(word, doc_id):
    """æŸ¥çœ‹æŒ‡å®šè¯æ±‡çš„æ‰€æœ‰å˜å½¢åŠå…¶é¢‘ç‡"""
    try:
        from core.engines.database.database_adapter import unified_adapter
        
        result = unified_adapter.get_word_variants(word, doc_id)
        
        click.echo(f"ğŸ” è¯æ±‡å˜å½¢åˆ†æ: \"{word}\"")
        click.echo("=" * 50)
        click.echo(f"ğŸŒ± è¯æ ¹: {result['lemma']}")
        click.echo(f"ğŸ“Š æ€»å˜å½¢æ•°: {result['total_variants']}")
        click.echo(f"ğŸ”¢ æ€»é¢‘ç‡: {result['total_frequency']}")
        
        if doc_id:
            click.echo(f"ğŸ“„ é™å®šæ–‡æ¡£: {doc_id[:8]}...")
        
        click.echo("\nğŸ“ è¯¦ç»†å˜å½¢ä¿¡æ¯:")
        click.echo("-" * 30)
        
        if result['variants']:
            for variant in result['variants']:
                surface = variant['surface_form']
                freq = variant.get('total_frequency', variant.get('frequency', 0))
                
                if doc_id:
                    # å•æ–‡æ¡£æ¨¡å¼
                    tf_score = variant.get('tf_score', 0)
                    filename = variant.get('filename', '')
                    click.echo(f"ğŸ“– {surface}: {freq}æ¬¡ (TF: {tf_score:.4f}) - {filename}")
                else:
                    # å…¨å±€æ¨¡å¼
                    doc_count = variant.get('document_count', 0)
                    avg_freq = variant.get('avg_frequency', 0)
                    click.echo(f"ğŸ“– {surface}: {freq}æ¬¡ (åœ¨ {doc_count} ä¸ªæ–‡æ¡£ä¸­, å¹³å‡ {avg_freq:.1f}æ¬¡/æ–‡æ¡£)")
        else:
            click.echo("âŒ æœªæ‰¾åˆ°ç›¸å…³å˜å½¢")
        
        click.echo("=" * 50)
        
    except Exception as e:
        click.secho(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}", fg='red', err=True)

@vocab.command()
@click.option('--doc-id', help='æŒ‡å®šæ–‡æ¡£IDè¿›è¡Œè¯æ ¹åˆ†æ')
@click.option('--limit', default=20, help='æ˜¾ç¤ºæ•°é‡é™åˆ¶')
def lemmas(doc_id, limit):
    """è¯æ ¹çº§åˆ«çš„ç»Ÿè®¡åˆ†æ"""
    try:
        from core.engines.database.database_adapter import unified_adapter
        
        result = unified_adapter.get_lemma_analysis_data(doc_id)
        
        click.echo("ğŸŒ± è¯æ ¹åˆ†ææŠ¥å‘Š")
        click.echo("=" * 50)
        
        if doc_id:
            click.echo(f"ğŸ“„ æ–‡æ¡£: {doc_id[:8]}...")
        else:
            click.echo("ğŸ“„ èŒƒå›´: å…¨éƒ¨æ–‡æ¡£")
        
        click.echo(f"ğŸ”¢ æ€»è¯æ ¹æ•°: {result['total_lemmas']}")
        
        click.echo(f"\nğŸ“Š Top {limit} é«˜é¢‘è¯æ ¹:")
        click.echo("-" * 40)
        
        for i, lemma_data in enumerate(result['lemma_analysis'][:limit], 1):
            lemma = lemma_data['lemma']
            variant_count = lemma_data['variant_count']
            total_freq = lemma_data['total_frequency']
            
            if doc_id:
                # å•æ–‡æ¡£æ¨¡å¼ - æ˜¾ç¤ºè¯¦ç»†å˜å½¢
                variants_detail = lemma_data.get('variants_detail', '')
                click.echo(f"{i:2d}. ğŸ“ {lemma} (æ€»é¢‘ç‡: {total_freq}, {variant_count} ä¸ªå˜å½¢)")
                if variants_detail:
                    variants = variants_detail.split(',')
                    variants_str = ', '.join(variants[:5])  # æœ€å¤šæ˜¾ç¤º5ä¸ªå˜å½¢
                    if len(variants) > 5:
                        variants_str += f", ... (+{len(variants)-5}ä¸ª)"
                    click.echo(f"     å˜å½¢: {variants_str}")
            else:
                # å…¨å±€æ¨¡å¼
                doc_count = lemma_data.get('document_count', 0)
                click.echo(f"{i:2d}. ğŸ“ {lemma}: {total_freq}æ¬¡ ({variant_count} ä¸ªå˜å½¢, {doc_count} ä¸ªæ–‡æ¡£)")
        
        click.echo("=" * 50)
        
    except Exception as e:
        click.secho(f"âŒ åˆ†æå¤±è´¥: {e}", fg='red', err=True)

@vocab.command()
@click.argument('word')
def pos(word):
    """æŸ¥çœ‹è¯æ±‡çš„è¯æ€§å’Œè¯­è¨€å­¦ç‰¹å¾"""
    try:
        from core.engines.database.database_adapter import unified_adapter
        
        results = unified_adapter.get_linguistic_features(word)
        
        if not results:
            click.secho(f"âŒ æœªæ‰¾åˆ°è¯æ±‡ \"{word}\" çš„è¯­è¨€å­¦ç‰¹å¾", fg='red')
            return
        
        click.echo(f"ğŸ” è¯æ±‡è¯­è¨€å­¦åˆ†æ: \"{word}\"")
        click.echo("=" * 50)
        
        for result in results:
            surface = result['surface_form']
            lemma = result['lemma']
            features = result.get('linguistic_features', {})
            
            click.echo(f"ğŸ“ è¯æ±‡å½¢å¼: {surface}")
            click.echo(f"ğŸŒ± è¯æ ¹: {lemma}")
            
            if features:
                # è¯æ€§ä¿¡æ¯
                pos_tag = features.get('pos_tag', 'UNKNOWN')
                pos_type = features.get('pos_type', 'unknown')
                pos_desc = features.get('pos_description', 'æ— æè¿°')
                
                click.echo(f"ğŸ·ï¸  è¯æ€§æ ‡ç­¾: {pos_tag}")
                click.echo(f"ğŸ“š è¯æ€§ç±»å‹: {pos_type}")
                click.echo(f"ğŸ“– è¯æ€§æè¿°: {pos_desc}")
                
                # å½¢æ€å­¦ä¿¡æ¯
                morphology = features.get('morphology', {})
                if morphology:
                    complexity = morphology.get('complexity', 'simple')
                    prefix = morphology.get('prefix')
                    suffix = morphology.get('suffix')
                    
                    click.echo(f"ğŸ”§ å½¢æ€å¤æ‚åº¦: {complexity}")
                    if prefix:
                        click.echo(f"â¬…ï¸  å‰ç¼€: {prefix}")
                    if suffix:
                        suffix_meaning = morphology.get('suffix_meaning', '')
                        click.echo(f"â¡ï¸  åç¼€: {suffix} ({suffix_meaning})")
                
                # å…¶ä»–ç‰¹å¾
                word_length = features.get('word_length', 0)
                has_prefix = features.get('has_prefix', False)
                has_suffix = features.get('has_suffix', False)
                
                click.echo(f"ğŸ“ è¯é•¿: {word_length} å­—ç¬¦")
                click.echo(f"ğŸ¯ æœ‰å‰ç¼€: {'æ˜¯' if has_prefix else 'å¦'}")
                click.echo(f"ğŸ¯ æœ‰åç¼€: {'æ˜¯' if has_suffix else 'å¦'}")
            else:
                click.echo("âŒ æ— è¯­è¨€å­¦ç‰¹å¾ä¿¡æ¯")
            
            click.echo("-" * 30)
        
        click.echo("=" * 50)
        
    except Exception as e:
        click.secho(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}", fg='red', err=True)

@vocab.command()
@click.option('--type', 'pos_type', help='è¯æ€§ç±»å‹ (noun, verb, adjective, ç­‰)')
@click.option('--limit', default=20, help='æ˜¾ç¤ºæ•°é‡é™åˆ¶')
def by_pos(pos_type, limit):
    """æŒ‰è¯æ€§ç±»å‹æŸ¥è¯¢è¯æ±‡"""
    try:
        from core.engines.database.database_adapter import unified_adapter
        
        if pos_type:
            # æŸ¥è¯¢ç‰¹å®šè¯æ€§çš„è¯æ±‡
            results = unified_adapter.get_words_by_pos(pos_type, limit)
            
            click.echo(f"ğŸ“š è¯æ€§ç±»å‹: {pos_type}")
            click.echo("=" * 50)
            
            if results:
                click.echo(f"å…±æ‰¾åˆ° {len(results)} ä¸ªè¯æ±‡:")
                click.echo()
                
                for i, word_data in enumerate(results, 1):
                    surface = word_data['surface_form']
                    lemma = word_data['lemma']
                    features = word_data.get('linguistic_features', {})
                    total_freq = word_data['total_frequency']
                    doc_count = word_data['document_count']
                    
                    pos_desc = features.get('pos_description', 'æ— æè¿°')
                    
                    click.echo(f"{i:2d}. ğŸ“ {surface} ({lemma})")
                    click.echo(f"     æè¿°: {pos_desc}")
                    click.echo(f"     é¢‘ç‡: {total_freq}æ¬¡ (åœ¨ {doc_count} ä¸ªæ–‡æ¡£ä¸­)")
                    
                    # æ˜¾ç¤ºå½¢æ€å­¦ä¿¡æ¯
                    morphology = features.get('morphology', {})
                    if morphology.get('complexity') != 'simple':
                        complexity_info = []
                        if morphology.get('prefix'):
                            complexity_info.append(f"å‰ç¼€:{morphology['prefix']}")
                        if morphology.get('suffix'):
                            complexity_info.append(f"åç¼€:{morphology['suffix']}")
                        if complexity_info:
                            click.echo(f"     å½¢æ€: {', '.join(complexity_info)}")
                    
                    click.echo()
            else:
                click.secho(f"âŒ æœªæ‰¾åˆ°è¯æ€§ä¸º \"{pos_type}\" çš„è¯æ±‡", fg='yellow')
        else:
            # æ˜¾ç¤ºè¯æ€§åˆ†å¸ƒç»Ÿè®¡
            stats = unified_adapter.get_pos_statistics()
            
            click.echo("ğŸ“Š è¯æ€§åˆ†å¸ƒç»Ÿè®¡")
            click.echo("=" * 50)
            
            total_analyzed = stats.get('total_analyzed_words', 0)
            click.echo(f"ğŸ“ å·²åˆ†æè¯æ±‡æ€»æ•°: {total_analyzed}")
            click.echo()
            
            # è¯æ€§ç±»å‹åˆ†å¸ƒ
            pos_type_dist = stats.get('pos_type_distribution', {})
            click.echo("ğŸ·ï¸  è¯æ€§ç±»å‹åˆ†å¸ƒ:")
            for pos_type, count in sorted(pos_type_dist.items(), key=lambda x: x[1], reverse=True):
                percentage = (count / total_analyzed * 100) if total_analyzed > 0 else 0
                click.echo(f"   {pos_type:<12}: {count:>4} ä¸ª ({percentage:>5.1f}%)")
            
            click.echo()
            
            # è¯¦ç»†è¯æ€§æ ‡ç­¾åˆ†å¸ƒ (å‰10ä¸ª)
            pos_tag_dist = stats.get('pos_tag_distribution', [])[:10]
            if pos_tag_dist:
                click.echo("ğŸ”– è¯¦ç»†è¯æ€§æ ‡ç­¾ (Top 10):")
                for item in pos_tag_dist:
                    tag = item['pos_tag']
                    desc = item['description']
                    count = item['count']
                    percentage = (count / total_analyzed * 100) if total_analyzed > 0 else 0
                    click.echo(f"   {tag:<6}: {count:>4} ä¸ª ({percentage:>5.1f}%) - {desc}")
            
            # å½¢æ€å­¦å¤æ‚åº¦åˆ†å¸ƒ
            morphology_dist = stats.get('morphology_distribution', {})
            if morphology_dist:
                click.echo()
                click.echo("ğŸ”§ å½¢æ€å­¦å¤æ‚åº¦åˆ†å¸ƒ:")
                for complexity, count in sorted(morphology_dist.items(), key=lambda x: x[1], reverse=True):
                    percentage = (count / total_analyzed * 100) if total_analyzed > 0 else 0
                    click.echo(f"   {complexity:<10}: {count:>4} ä¸ª ({percentage:>5.1f}%)")
        
        click.echo("=" * 50)
        
    except Exception as e:
        click.secho(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}", fg='red', err=True)

@vocab.command()
def morphology():
    """å½¢æ€å­¦åˆ†æ - æ˜¾ç¤ºæœ‰å‰ç¼€å’Œåç¼€çš„è¯æ±‡"""
    try:
        from core.engines.database.database_adapter import unified_adapter
        
        analysis = unified_adapter.get_morphology_analysis()
        
        click.echo("ğŸ”§ å½¢æ€å­¦åˆ†ææŠ¥å‘Š")
        click.echo("=" * 60)
        
        # å‰ç¼€è¯æ±‡
        prefixed_words = analysis.get('prefixed_words', [])
        if prefixed_words:
            click.echo("â¬…ï¸  å‰ç¼€è¯æ±‡ (Top 20):")
            click.echo("-" * 30)
            
            for word_data in prefixed_words:
                surface = word_data['surface_form']
                lemma = word_data['lemma']
                prefix = word_data['prefix']
                freq = word_data['total_frequency']
                
                click.echo(f"ğŸ“ {surface} ({lemma})")
                click.echo(f"   å‰ç¼€: {prefix} | é¢‘ç‡: {freq}æ¬¡")
                click.echo()
        
        # åç¼€è¯æ±‡
        suffixed_words = analysis.get('suffixed_words', [])
        if suffixed_words:
            click.echo("â¡ï¸  åç¼€è¯æ±‡ (Top 20):")
            click.echo("-" * 30)
            
            for word_data in suffixed_words:
                surface = word_data['surface_form']
                lemma = word_data['lemma']
                suffix = word_data['suffix']
                suffix_meaning = word_data['suffix_meaning']
                freq = word_data['total_frequency']
                
                click.echo(f"ğŸ“ {surface} ({lemma})")
                click.echo(f"   åç¼€: {suffix} ({suffix_meaning}) | é¢‘ç‡: {freq}æ¬¡")
                click.echo()
        
        if not prefixed_words and not suffixed_words:
            click.secho("âŒ æš‚æ— å½¢æ€å­¦å¤æ‚è¯æ±‡æ•°æ®", fg='yellow')
        
        click.echo("=" * 60)
        
    except Exception as e:
        click.secho(f"âŒ åˆ†æå¤±è´¥: {e}", fg='red', err=True)

@vocab.command()
def tags():
    """æ˜¾ç¤ºæ‰€æœ‰è¯æ±‡è¡¨æ ‡ç­¾"""
    try:
        from core.engines.database.database_adapter import unified_adapter
        
        wordlists = unified_adapter.get_all_wordlists()
        
        if not wordlists:
            click.echo("ğŸ“š æš‚æ— è¯æ±‡è¡¨")
            return
        
        click.echo("ğŸ·ï¸  è¯æ±‡è¡¨:")
        click.echo("-" * 60)
        for wl in wordlists:
            click.echo(f"ğŸ“š {wl['name']}")
        click.echo("-" * 60)
        click.echo(f"æ€»è®¡ {len(wordlists)} ä¸ªè¯æ±‡è¡¨")
        
    except Exception as e:
        click.secho(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}", fg='red', err=True)

# =============================================================================
# âš™ï¸ CONFIG å‘½ä»¤ç»„ - é…ç½®ç®¡ç†
# =============================================================================

@cli.group('config')
def config_cmd():
    """é…ç½®ç®¡ç†å‘½ä»¤"""
    pass

@config_cmd.command()
def show():
    """æ˜¾ç¤ºå½“å‰é…ç½®"""
    try:
        config = get_config()
        
        click.echo("âš™ï¸  å½“å‰é…ç½®:")
        click.echo("-" * 40)
        
        def print_config(data, prefix=""):
            for key, value in data.items():
                if isinstance(value, dict):
                    click.echo(f"{prefix}{key}:")
                    print_config(value, prefix + "  ")
                else:
                    click.echo(f"{prefix}{key}: {value}")
        
        print_config(config)
        
    except Exception as e:
        click.secho(f"âŒ é…ç½®æ˜¾ç¤ºå¤±è´¥: {e}", fg='red', err=True)

@config_cmd.command()
@click.argument('key')
@click.argument('value')
def set(key, value):
    """è®¾ç½®é…ç½®é¡¹"""
    try:
        config = get_config()
        config.update(key, value)
        click.secho(f"âœ… é…ç½®å·²æ›´æ–°: {key} = {value}", fg='green')
        
    except Exception as e:
        click.secho(f"âŒ é…ç½®è®¾ç½®å¤±è´¥: {e}", fg='red', err=True)

def main():
    """CLIä¸»ç¨‹åºå…¥å£å‡½æ•°"""
    try:
        cli()
    except KeyboardInterrupt:
        click.secho("\nğŸ‘‹ ç¨‹åºå·²é€€å‡º", fg='yellow')
    except Exception as e:
        click.secho(f"\nâŒ ç¨‹åºå‡ºé”™: {str(e)}", fg='red', err=True)
        sys.exit(1)

if __name__ == '__main__':
    main() 