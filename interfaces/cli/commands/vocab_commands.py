# è¯æ±‡æŸ¥è¯¢å‘½ä»¤æ¨¡å—
# è·¯å¾„: interfaces/cli/commands/vocab_commands.py
# é¡¹ç›®åç§°: Word Frequency Analysis
# ä½œè€…: Sherryyue

import click

@click.group()
def vocab():
    """è¯æ±‡æŸ¥è¯¢å’Œç®¡ç†ç›¸å…³å‘½ä»¤
    
    æŸ¥è¯¢è¯æ±‡é¢‘ç‡ã€è¯æ±‡è¡¨ä¿¡æ¯ã€ç»Ÿè®¡æ•°æ®ç­‰ã€‚
    """
    pass

@vocab.command()
@click.argument('word')
@click.option('-d', '--detailed', is_flag=True, help='æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯ï¼ˆåŒ…å«è¯æ€§ã€å­—å…¸æ’åç­‰ï¼‰')
@click.option('--format', 'output_format', type=click.Choice(['text', 'json']), default='text', help='è¾“å‡ºæ ¼å¼')
def query(word, detailed, output_format):
    """æŸ¥è¯¢è¯æ±‡ä¿¡æ¯"""
    try:
        from core.engines.database.database_adapter import unified_adapter
        
        # æŸ¥è¯¢è¯æ±‡é¢‘ç‡ï¼ˆä½¿ç”¨detailedå‚æ•°ï¼‰
        results = unified_adapter.search_words(word, detailed=detailed)
        
        if not results:
            click.echo(f"âŒ æœªæ‰¾åˆ°åŒ…å« '{word}' çš„è¯æ±‡")
            return
        
        if output_format == 'json':
            import json
            if detailed:
                data = [{
                    "word": r[0], 
                    "total_frequency": r[1], 
                    "document_count": r[2],
                    "pos_primary": r[3],
                    "dictionary_rank": r[4],
                    "personal_status": r[5],
                    "definition": r[6][:100] + "..." if r[6] and len(r[6]) > 100 else r[6]
                } for r in results]
            else:
                data = [{"word": r[0], "total_frequency": r[1], "document_count": r[2]} for r in results]
            click.echo(json.dumps(data, ensure_ascii=False, indent=2))
        else:
            if detailed:
                click.echo(f"ğŸ” åŒ…å« '{word}' çš„è¯æ±‡ (è¯¦ç»†æ¨¡å¼):")
                click.echo("=" * 60)
                for result in results[:20]:  # é™åˆ¶æ˜¾ç¤ºå‰20ä¸ª
                    word_text, freq, doc_count, pos_primary, dict_rank, personal_status, definition = result
                    
                    # æ ¼å¼åŒ–è¯æ€§å’Œæ’åä¿¡æ¯
                    pos_info = f"({pos_primary})" if pos_primary else "(æœªçŸ¥è¯æ€§)"
                    rank_info = f"å­—å…¸æ’å:{dict_rank}" if dict_rank else "æœªåœ¨å­—å…¸ä¸­"
                    status_icon = {'new': 'ğŸ†•', 'learn': 'ğŸ“š', 'know': 'âœ…', 'master': 'ğŸ†'}.get(personal_status, 'âšª')
                    status_info = f"{status_icon}{personal_status}" if personal_status else "âšªæœªè®¾ç½®"
                    
                    click.echo(f"ğŸ“ {word_text} {pos_info}")
                    click.echo(f"   ğŸ“Š é¢‘ç‡: {freq}, æ–‡æ¡£: {doc_count}ä¸ª")
                    click.echo(f"   ğŸ“š {rank_info} | å­¦ä¹ çŠ¶æ€: {status_info}")
                    
                    if definition:
                        def_preview = definition[:80] + "..." if len(definition) > 80 else definition
                        click.echo(f"   ğŸ’¡ å®šä¹‰: {def_preview}")
                    
                    click.echo()  # ç©ºè¡Œåˆ†éš”
            else:
                click.echo(f"ğŸ” åŒ…å« '{word}' çš„è¯æ±‡:")
                click.echo("-" * 50)
                for result in results[:20]:  # é™åˆ¶æ˜¾ç¤ºå‰20ä¸ª
                    word_text, freq, doc_count = result
                    click.echo(f"ğŸ“ {word_text}: é¢‘ç‡ {freq}, å‡ºç°åœ¨ {doc_count} ä¸ªæ–‡æ¡£")
            
            if len(results) > 20:
                click.echo(f"... è¿˜æœ‰ {len(results) - 20} ä¸ªç›¸å…³è¯æ±‡")
                click.echo("ğŸ’¡ ä½¿ç”¨ --detailed æŸ¥çœ‹æ›´å¤šä¿¡æ¯")
                
    except Exception as e:
        click.secho(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}", fg='red', err=True)

@vocab.command()
def stats():
    """æ˜¾ç¤ºè¯æ±‡ç»Ÿè®¡ä¿¡æ¯"""
    try:
        from core.engines.database.database_adapter import unified_adapter
        
        stats = unified_adapter.get_database_info()
        
        click.echo("ğŸ“Š è¯æ±‡ç»Ÿè®¡:")
        click.echo("-" * 40)
        click.echo(f"ğŸ“š è¯æ±‡è¡¨æ•°é‡: {stats.get('wordlists_count', 0)}")
        click.echo(f"ğŸ”¤ æ€»è¯æ±‡æ•° (åŸå§‹å½¢å¼): {stats.get('total_words', 0)}")
        click.echo(f"ğŸŒ± ç‹¬ç‰¹è¯æ ¹æ•°: {stats.get('unique_lemmas', 0)}")
        click.echo(f"ğŸ“„ æ–‡æ¡£æ•°é‡: {stats.get('documents_count', 0)}")
        click.echo(f"ğŸ“ è¯é¢‘è®°å½•: {stats.get('word_frequencies_count', 0)}")
        
        if stats.get('documents_count', 0) > 0:
            avg_length = stats.get('avg_document_length', 0)
            if avg_length:
                click.echo(f"ğŸ“ å¹³å‡æ–‡æ¡£é•¿åº¦: {avg_length:.1f} è¯")
        
        avg_variants = stats.get('avg_variants_per_lemma', 0)
        if avg_variants > 0:
            click.echo(f"ğŸ”„ å¹³å‡æ¯è¯æ ¹å˜å½¢æ•°: {avg_variants:.1f}")
        click.echo("-" * 40)
        
    except Exception as e:
        click.secho(f"âŒ ç»Ÿè®¡å¤±è´¥: {e}", fg='red', err=True)

@vocab.command()
@click.argument('word')
@click.option('--doc-id', help='æŒ‡å®šæ–‡æ¡£IDæŸ¥çœ‹ç‰¹å®šæ–‡æ¡£ä¸­çš„å˜å½¢')
def variants(word, doc_id):
    """æŸ¥çœ‹æŒ‡å®šè¯æ±‡çš„æ‰€æœ‰å˜å½¢åŠå…¶é¢‘ç‡"""
    try:
        from core.engines.database.database_adapter import unified_adapter
        
        result = unified_adapter.get_word_variants(word, doc_id)
        
        click.echo(f"ğŸ” è¯æ±‡å˜å½¢åˆ†æ: \"{word}\"")
        click.echo("=" * 50)
        click.echo(f"ğŸŒ± è¯æ ¹: {result['lemma']}")
        click.echo(f"ğŸ“Š æ€»å˜å½¢æ•°: {result['total_variants']}")
        click.echo(f"ğŸ”¢ æ€»é¢‘ç‡: {result['total_frequency']}")
        
        if doc_id:
            click.echo(f"ğŸ“„ é™å®šæ–‡æ¡£: {doc_id[:8]}...")
        
        click.echo("\nğŸ“ è¯¦ç»†å˜å½¢ä¿¡æ¯:")
        click.echo("-" * 30)
        
        if result['variants']:
            for variant in result['variants']:
                surface = variant['surface_form']
                freq = variant.get('total_frequency', variant.get('frequency', 0))
                
                if doc_id:
                    # å•æ–‡æ¡£æ¨¡å¼
                    tf_score = variant.get('tf_score', 0)
                    filename = variant.get('filename', '')
                    click.echo(f"ğŸ“– {surface}: {freq}æ¬¡ (TF: {tf_score:.4f}) - {filename}")
                else:
                    # å…¨å±€æ¨¡å¼
                    doc_count = variant.get('document_count', 0)
                    avg_freq = variant.get('avg_frequency', 0)
                    click.echo(f"ğŸ“– {surface}: {freq}æ¬¡ (åœ¨ {doc_count} ä¸ªæ–‡æ¡£ä¸­, å¹³å‡ {avg_freq:.1f}æ¬¡/æ–‡æ¡£)")
        else:
            click.echo("âŒ æœªæ‰¾åˆ°ç›¸å…³å˜å½¢")
        
        click.echo("=" * 50)
        
    except Exception as e:
        click.secho(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}", fg='red', err=True)

@vocab.command()
@click.option('--doc-id', help='æŒ‡å®šæ–‡æ¡£IDè¿›è¡Œè¯æ ¹åˆ†æ')
@click.option('--limit', default=20, help='æ˜¾ç¤ºæ•°é‡é™åˆ¶')
def lemmas(doc_id, limit):
    """è¯æ ¹çº§åˆ«çš„ç»Ÿè®¡åˆ†æ"""
    try:
        from core.engines.database.database_adapter import unified_adapter
        
        result = unified_adapter.get_lemma_analysis_data(doc_id)
        
        click.echo("ğŸŒ± è¯æ ¹åˆ†ææŠ¥å‘Š")
        click.echo("=" * 50)
        
        if doc_id:
            click.echo(f"ğŸ“„ æ–‡æ¡£: {doc_id[:8]}...")
        else:
            click.echo("ğŸ“„ èŒƒå›´: å…¨éƒ¨æ–‡æ¡£")
        
        click.echo(f"ğŸ”¢ æ€»è¯æ ¹æ•°: {result['total_lemmas']}")
        
        click.echo(f"\nğŸ“Š Top {limit} é«˜é¢‘è¯æ ¹:")
        click.echo("-" * 40)
        
        for i, lemma_data in enumerate(result['lemma_analysis'][:limit], 1):
            lemma = lemma_data['lemma']
            variant_count = lemma_data['variant_count']
            total_freq = lemma_data['total_frequency']
            
            if doc_id:
                # å•æ–‡æ¡£æ¨¡å¼ - æ˜¾ç¤ºè¯¦ç»†å˜å½¢
                variants_detail = lemma_data.get('variants_detail', '')
                click.echo(f"{i:2d}. ğŸ“ {lemma} (æ€»é¢‘ç‡: {total_freq}, {variant_count} ä¸ªå˜å½¢)")
                if variants_detail:
                    variants = variants_detail.split(',')
                    variants_str = ', '.join(variants[:5])  # æœ€å¤šæ˜¾ç¤º5ä¸ªå˜å½¢
                    if len(variants) > 5:
                        variants_str += f", ... (+{len(variants)-5}ä¸ª)"
                    click.echo(f"     å˜å½¢: {variants_str}")
            else:
                # å…¨å±€æ¨¡å¼
                doc_count = lemma_data.get('document_count', 0)
                click.echo(f"{i:2d}. ğŸ“ {lemma}: {total_freq}æ¬¡ ({variant_count} ä¸ªå˜å½¢, {doc_count} ä¸ªæ–‡æ¡£)")
        
        click.echo("=" * 50)
        
    except Exception as e:
        click.secho(f"âŒ åˆ†æå¤±è´¥: {e}", fg='red', err=True)

@vocab.command()
@click.argument('word')
def pos(word):
    """æŸ¥çœ‹è¯æ±‡çš„è¯æ€§å’Œè¯­è¨€å­¦ç‰¹å¾"""
    try:
        from core.engines.database.database_adapter import unified_adapter
        
        results = unified_adapter.get_linguistic_features(word)
        
        if not results:
            click.secho(f"âŒ æœªæ‰¾åˆ°è¯æ±‡ \"{word}\" çš„è¯­è¨€å­¦ç‰¹å¾", fg='red')
            return
        
        click.echo(f"ğŸ” è¯æ±‡è¯­è¨€å­¦åˆ†æ: \"{word}\"")
        click.echo("=" * 50)
        
        for result in results:
            surface = result['surface_form']
            lemma = result['lemma']
            features = result.get('linguistic_features', {})
            
            click.echo(f"ğŸ“ è¯æ±‡å½¢å¼: {surface}")
            click.echo(f"ğŸŒ± è¯æ ¹: {lemma}")
            
            if features:
                # è¯æ€§ä¿¡æ¯
                pos_tag = features.get('pos_tag', 'UNKNOWN')
                pos_type = features.get('pos_type', 'unknown')
                pos_desc = features.get('pos_description', 'æ— æè¿°')
                
                click.echo(f"ğŸ·ï¸  è¯æ€§æ ‡ç­¾: {pos_tag}")
                click.echo(f"ğŸ“š è¯æ€§ç±»å‹: {pos_type}")
                click.echo(f"ğŸ“– è¯æ€§æè¿°: {pos_desc}")
                
                # å½¢æ€å­¦ä¿¡æ¯
                morphology = features.get('morphology', {})
                if morphology:
                    complexity = morphology.get('complexity', 'simple')
                    prefix = morphology.get('prefix')
                    suffix = morphology.get('suffix')
                    
                    click.echo(f"ğŸ”§ å½¢æ€å¤æ‚åº¦: {complexity}")
                    if prefix:
                        click.echo(f"â¬…ï¸  å‰ç¼€: {prefix}")
                    if suffix:
                        suffix_meaning = morphology.get('suffix_meaning', '')
                        click.echo(f"â¡ï¸  åç¼€: {suffix} ({suffix_meaning})")
                
                # å…¶ä»–ç‰¹å¾
                word_length = features.get('word_length', 0)
                has_prefix = features.get('has_prefix', False)
                has_suffix = features.get('has_suffix', False)
                
                click.echo(f"ğŸ“ è¯é•¿: {word_length} å­—ç¬¦")
                click.echo(f"ğŸ¯ æœ‰å‰ç¼€: {'æ˜¯' if has_prefix else 'å¦'}")
                click.echo(f"ğŸ¯ æœ‰åç¼€: {'æ˜¯' if has_suffix else 'å¦'}")
            else:
                click.echo("âŒ æ— è¯­è¨€å­¦ç‰¹å¾ä¿¡æ¯")
            
            click.echo("-" * 30)
        
        click.echo("=" * 50)
        
    except Exception as e:
        click.secho(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}", fg='red', err=True)

@vocab.command()
@click.option('--type', 'pos_type', help='è¯æ€§ç±»å‹ (noun, verb, adjective, ç­‰)')
@click.option('--limit', default=20, help='æ˜¾ç¤ºæ•°é‡é™åˆ¶')
def by_pos(pos_type, limit):
    """æŒ‰è¯æ€§ç±»å‹æŸ¥è¯¢è¯æ±‡"""
    try:
        from core.engines.database.database_adapter import unified_adapter
        
        if pos_type:
            # æŸ¥è¯¢ç‰¹å®šè¯æ€§çš„è¯æ±‡
            results = unified_adapter.get_words_by_pos(pos_type, limit)
            
            click.echo(f"ğŸ“š è¯æ€§ç±»å‹: {pos_type}")
            click.echo("=" * 50)
            
            if results:
                click.echo(f"å…±æ‰¾åˆ° {len(results)} ä¸ªè¯æ±‡:")
                click.echo()
                
                for i, word_data in enumerate(results, 1):
                    surface = word_data['surface_form']
                    lemma = word_data['lemma']
                    features = word_data.get('linguistic_features', {})
                    total_freq = word_data['total_frequency']
                    doc_count = word_data['document_count']
                    
                    pos_desc = features.get('pos_description', 'æ— æè¿°')
                    
                    click.echo(f"{i:2d}. ğŸ“ {surface} ({lemma})")
                    click.echo(f"     æè¿°: {pos_desc}")
                    click.echo(f"     é¢‘ç‡: {total_freq}æ¬¡ (åœ¨ {doc_count} ä¸ªæ–‡æ¡£ä¸­)")
                    
                    # æ˜¾ç¤ºå½¢æ€å­¦ä¿¡æ¯
                    morphology = features.get('morphology', {})
                    if morphology.get('complexity') != 'simple':
                        complexity_info = []
                        if morphology.get('prefix'):
                            complexity_info.append(f"å‰ç¼€:{morphology['prefix']}")
                        if morphology.get('suffix'):
                            complexity_info.append(f"åç¼€:{morphology['suffix']}")
                        if complexity_info:
                            click.echo(f"     å½¢æ€: {', '.join(complexity_info)}")
                    
                    click.echo()
            else:
                click.secho(f"âŒ æœªæ‰¾åˆ°è¯æ€§ä¸º \"{pos_type}\" çš„è¯æ±‡", fg='yellow')
        else:
            # æ˜¾ç¤ºè¯æ€§åˆ†å¸ƒç»Ÿè®¡
            stats = unified_adapter.get_pos_statistics()
            
            click.echo("ğŸ“Š è¯æ€§åˆ†å¸ƒç»Ÿè®¡")
            click.echo("=" * 50)
            
            total_analyzed = stats.get('total_analyzed_words', 0)
            click.echo(f"ğŸ“ å·²åˆ†æè¯æ±‡æ€»æ•°: {total_analyzed}")
            click.echo()
            
            # è¯æ€§ç±»å‹åˆ†å¸ƒ
            pos_type_dist = stats.get('pos_type_distribution', {})
            click.echo("ğŸ·ï¸  è¯æ€§ç±»å‹åˆ†å¸ƒ:")
            for pos_type, count in sorted(pos_type_dist.items(), key=lambda x: x[1], reverse=True):
                percentage = (count / total_analyzed * 100) if total_analyzed > 0 else 0
                click.echo(f"   {pos_type:<12}: {count:>4} ä¸ª ({percentage:>5.1f}%)")
            
            click.echo()
            
            # è¯¦ç»†è¯æ€§æ ‡ç­¾åˆ†å¸ƒ (å‰10ä¸ª)
            pos_tag_dist = stats.get('pos_tag_distribution', [])[:10]
            if pos_tag_dist:
                click.echo("ğŸ”– è¯¦ç»†è¯æ€§æ ‡ç­¾ (Top 10):")
                for item in pos_tag_dist:
                    tag = item['pos_tag']
                    desc = item['description']
                    count = item['count']
                    percentage = (count / total_analyzed * 100) if total_analyzed > 0 else 0
                    click.echo(f"   {tag:<6}: {count:>4} ä¸ª ({percentage:>5.1f}%) - {desc}")
            
            # å½¢æ€å­¦å¤æ‚åº¦åˆ†å¸ƒ
            morphology_dist = stats.get('morphology_distribution', {})
            if morphology_dist:
                click.echo()
                click.echo("ğŸ”§ å½¢æ€å­¦å¤æ‚åº¦åˆ†å¸ƒ:")
                for complexity, count in sorted(morphology_dist.items(), key=lambda x: x[1], reverse=True):
                    percentage = (count / total_analyzed * 100) if total_analyzed > 0 else 0
                    click.echo(f"   {complexity:<10}: {count:>4} ä¸ª ({percentage:>5.1f}%)")
        
        click.echo("=" * 50)
        
    except Exception as e:
        click.secho(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}", fg='red', err=True)

@vocab.command()
def morphology():
    """å½¢æ€å­¦åˆ†æ - æ˜¾ç¤ºæœ‰å‰ç¼€å’Œåç¼€çš„è¯æ±‡"""
    try:
        from core.engines.database.database_adapter import unified_adapter
        
        analysis = unified_adapter.get_morphology_analysis()
        
        click.echo("ğŸ”§ å½¢æ€å­¦åˆ†ææŠ¥å‘Š")
        click.echo("=" * 60)
        
        # å‰ç¼€è¯æ±‡
        prefixed_words = analysis.get('prefixed_words', [])
        if prefixed_words:
            click.echo("â¬…ï¸  å‰ç¼€è¯æ±‡ (Top 20):")
            click.echo("-" * 30)
            
            for word_data in prefixed_words:
                surface = word_data['surface_form']
                lemma = word_data['lemma']
                prefix = word_data['prefix']
                freq = word_data['total_frequency']
                
                click.echo(f"ğŸ“ {surface} ({lemma})")
                click.echo(f"   å‰ç¼€: {prefix} | é¢‘ç‡: {freq}æ¬¡")
                click.echo()
        
        # åç¼€è¯æ±‡
        suffixed_words = analysis.get('suffixed_words', [])
        if suffixed_words:
            click.echo("â¡ï¸  åç¼€è¯æ±‡ (Top 20):")
            click.echo("-" * 30)
            
            for word_data in suffixed_words:
                surface = word_data['surface_form']
                lemma = word_data['lemma']
                suffix = word_data['suffix']
                suffix_meaning = word_data['suffix_meaning']
                freq = word_data['total_frequency']
                
                click.echo(f"ğŸ“ {surface} ({lemma})")
                click.echo(f"   åç¼€: {suffix} ({suffix_meaning}) | é¢‘ç‡: {freq}æ¬¡")
                click.echo()
        
        if not prefixed_words and not suffixed_words:
            click.secho("âŒ æš‚æ— å½¢æ€å­¦å¤æ‚è¯æ±‡æ•°æ®", fg='yellow')
        
        click.echo("=" * 60)
        
    except Exception as e:
        click.secho(f"âŒ åˆ†æå¤±è´¥: {e}", fg='red', err=True)

@vocab.command()
def tags():
    """æ˜¾ç¤ºæ‰€æœ‰è¯æ±‡è¡¨æ ‡ç­¾"""
    try:
        from core.engines.database.database_adapter import unified_adapter
        
        wordlists = unified_adapter.get_all_wordlists()
        
        if not wordlists:
            click.echo("ğŸ“š æš‚æ— è¯æ±‡è¡¨")
            return
        
        click.echo("ğŸ·ï¸  è¯æ±‡è¡¨:")
        click.echo("-" * 60)
        for wl in wordlists:
            click.echo(f"ğŸ“š {wl['name']}")
        click.echo("-" * 60)
        click.echo(f"æ€»è®¡ {len(wordlists)} ä¸ªè¯æ±‡è¡¨")
        
    except Exception as e:
        click.secho(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}", fg='red', err=True) 