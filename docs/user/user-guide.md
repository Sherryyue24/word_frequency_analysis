# 用户使用手册

## 📖 目录

1. [安装和启动](#安装和启动)
2. [主界面介绍](#主界面介绍)
3. [文本分析功能](#文本分析功能)
4. [词汇管理功能](#词汇管理功能)
5. [数据导出](#数据导出)
6. [常见问题](#常见问题)

---

## 安装和启动

### 系统要求
- Python 3.8 或更高版本
- 操作系统: Windows、macOS、Linux
- 内存: 建议 2GB 以上

### 安装步骤

1. **下载项目**
   ```bash
   git clone [项目地址]
   cd word-frequency-analysis
   ```

2. **安装依赖**
   ```bash
   pip install -r requirements.txt
   ```

3. **启动程序**
   ```bash
   python run.py
   ```

### 首次启动
程序首次启动时会自动创建必要的数据库和配置文件。

---

## 主界面介绍

启动程序后，您会看到主菜单：

```
=== 文本处理与词汇管理系统 ===
1. 文本处理
2. 词汇管理
0. 退出程序
```

### 功能模块说明

- **文本处理**: 分析文档词频，生成统计报告
- **词汇管理**: 查询单词信息，管理词汇表

---

## 文本分析功能

### 3.1 处理新文本

1. **选择功能**
   - 主菜单选择 `1. 文本处理`
   - 子菜单选择 `1. 处理新文本`

2. **输入文件路径**
   ```
   请输入要处理的文件夹路径: /path/to/your/documents
   ```

3. **支持的文件格式**
   - `.txt` - 纯文本文件
   - `.pdf` - PDF文档
   - `.docx` - Word文档
   - `.csv` - CSV文件

4. **处理过程**
   - 程序会扫描指定文件夹及子文件夹
   - 自动识别支持的文件格式
   - 显示处理进度和结果

### 3.2 查询分析结果

1. **查看文本列表**
   - 选择 `3. 查询数据库` → `1. 查看所有文本列表`
   - 显示所有已分析的文本及其ID

2. **查看特定文本词频**
   - 选择 `2. 查看特定文本的词频`
   - 输入文本ID
   - 查看详细的词频统计

3. **查看总体统计**
   - 选择 `3. 查看总体词频统计`
   - 设置最小频率阈值
   - 查看跨文档的词频统计

### 3.3 删除分析记录

- 选择 `2. 删除文本`
- 查看现有文本列表
- 输入要删除的文本ID（支持批量删除）

---

## 词汇管理功能

### 4.1 查询单个单词

1. **基本查询**
   - 选择 `2. 词汇管理` → `1. 查询单个单词`
   - 输入要查询的单词
   
2. **查询结果包含**
   - **词元** (Lemma): 单词的基本形式
   - **词性**: 详细的语法分类
   - **词形变化**: 复数、时态变化等
   - **派生词**: 相关的衍生词汇
   - **标签**: 所属的词汇表分类

3. **示例查询结果**
   ```
   === 查询结果 ===
   词元 (Lemma): analyze
   
   词性 (Parts of Speech):
   动词原形 (Verb, base form)
   
   词形变化 (Word Forms):
   - analyze
   - analyzes
   - analyzing
   - analyzed
   
   派生词 (Derivatives):
   - analysis
   - analytical
   - analyzer
   
   标签 (Tags):
   - awl
   ```

### 4.2 管理词汇表

1. **查看所有标签**
   - 选择 `2. 显示所有标签`
   - 显示所有词汇表及其包含的单词数量

2. **查询特定标签下的单词**
   - 选择 `3. 查询特定标签下的所有单词`
   - 输入标签名（如: awl, gsl）
   - 结果会保存到文件，显示保存位置

3. **导入新词表**
   - 选择 `4. 从文件导入词表`
   - 输入词表文件路径
   - 支持的格式：每行一个单词的文本文件

### 4.3 词汇表说明

| 标签 | 全称 | 说明 | 词汇量 |
|------|------|------|--------|
| gsl | General Service List | 最常用英语词汇 | ~2000词 |
| awl | Academic Word List | 学术英语词汇 | ~570词 |
| oxford3000 | Oxford 3000 | 牛津最重要3000词 | 3000词 |
| toefl | TOEFL Core | 托福核心词汇 | 变化 |
| ielts | IELTS Core | 雅思核心词汇 | 变化 |

---

## 数据导出

### 5.1 自动导出

程序会自动将以下内容导出到文件：

1. **词汇表查询结果**
   - 文件名格式: `tag_[标签名]_[时间戳].txt`
   - 位置: 项目根目录

2. **分析报告**
   - 导入词表时的详细报告
   - 文件名格式: `[词表文件名]_import_report.txt`

### 5.2 数据位置

- **数据库文件**: `data/databases/`
- **输入文件**: `data/files/new/` (文本文件和词汇表文件)
- **导出文件**: `data/exports/`
- **缓存文件**: `data/cache/`

---

## 常见问题

### 6.1 安装问题

**Q: 安装依赖时报错？**
```bash
# 尝试升级pip
pip install --upgrade pip

# 使用清华源
pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple/
```

**Q: 导入NLTK错误？**
```python
# 手动下载NLTK数据
import nltk
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger_en')
```

### 6.2 使用问题

**Q: 无法处理中文文档？**
A: 目前系统主要支持英文文本分析，中文支持有限。

**Q: PDF文件无法读取？**
A: 某些加密或特殊格式的PDF可能无法处理，建议转换为文本格式。

**Q: 词汇查询无结果？**
A: 请确保：
1. 已导入相关词汇表
2. 单词拼写正确
3. 尝试查询单词的基本形式

**Q: 处理大文件很慢？**
A: 
1. 系统会自动缓存结果，重复处理会更快
2. 大文件建议分批处理
3. 确保足够的内存空间

### 6.3 数据管理

**Q: 如何备份数据？**
A: 备份 `data/` 文件夹即可保存所有数据。

**Q: 如何重置系统？**
A: 删除 `data/databases/` 下的数据库文件，重启程序会重新初始化。

### 6.4 获取帮助

- **技术问题**: 提交 GitHub Issue
- **使用疑问**: 查看项目文档
- **联系作者**: Sherryyue

---

## 📝 使用技巧

1. **批量处理**: 将多个文档放在同一文件夹中一次性处理
2. **词汇对比**: 先导入标准词表，再分析文本，了解词汇水平
3. **定期备份**: 重要分析结果建议定期备份
4. **合理命名**: 给文件和文件夹使用有意义的名称，便于管理

---

*本手册持续更新，如有疑问请参考最新版本或联系支持。* 