# æ–‡ä»¶å¤„ç†å™¨ - ä½¿ç”¨ç»Ÿä¸€æ•°æ®åº“é€‚é…å™¨
# è·¯å¾„: core/engines/file_processor.py
# é¡¹ç›®åç§°: Word Frequency Analysis
# ä½œè€…: Sherryyue

from pathlib import Path
from datetime import datetime
from ..vocabulary.word_analyzer import analyze_text
from .file_reader import TextReader
from ..database.database_adapter import unified_adapter
from ...utils.helpers import get_supported_files
import os
import time
import shutil
from collections import Counter

class TextProcessor:
    """è´Ÿè´£å¤„ç†æ–°æ–‡æœ¬æ–‡ä»¶çš„ç±» - ç°å·²ä½¿ç”¨ç»Ÿä¸€æ¶æ„"""
    def __init__(self, storage_manager=None, move_processed=True):
        self.reader = TextReader()  # ç»„åˆå…³ç³»
        # ä½¿ç”¨ä¼ å…¥çš„å­˜å‚¨ç®¡ç†å™¨æˆ–é»˜è®¤çš„ç»Ÿä¸€é€‚é…å™¨
        self.storage_manager = storage_manager or unified_adapter
        # æ˜¯å¦åœ¨å¤„ç†å®Œæˆåç§»åŠ¨æ–‡ä»¶åˆ°processedç›®å½•
        self.move_processed = move_processed
    
    def process_new_texts(self, directory_path, scan_subdirs=True):
        """å¤„ç†æŒ‡å®šç›®å½•ä¸‹çš„æ–°æ–‡æœ¬æ–‡ä»¶"""
        try:
            file_paths = get_supported_files(directory_path, scan_subdirs, 
                                           supported_formats=self.reader.supported_formats.keys())
            if not self._validate_files(file_paths, directory_path):
                return
                
            self._process_files(file_paths, directory_path)
            # ç»Ÿä¸€æ¶æ„ä¸‹ä¸éœ€è¦æ‰‹åŠ¨æ›´æ–°è¯é¢‘ç»Ÿè®¡
            print("âœ… æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆ")
            
        except Exception as e:
            print(f"å¤„ç†æ–‡æœ¬æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
    
    def _validate_files(self, file_paths, directory_path):
        """éªŒè¯æ‰¾åˆ°çš„æ–‡ä»¶"""
        if not file_paths:
            print(f"åœ¨ç›®å½• {directory_path} ä¸­æ²¡æœ‰æ‰¾åˆ°æ”¯æŒçš„æ–‡ä»¶ç±»å‹")
            return False
            
        print(f"\næ‰¾åˆ° {len(file_paths)} ä¸ªæ–‡ä»¶å¾…å¤„ç†:")
        for path in file_paths:
            rel_path = os.path.relpath(path, directory_path)
            print(f"- {rel_path}")
        return True
    
    def _process_files(self, file_paths, directory_path):
        """å¤„ç†æ–‡ä»¶åˆ—è¡¨"""
        print("\nå¼€å§‹å¤„ç†æ–‡ä»¶...")
        for i, file_path in enumerate(file_paths, 1):
            rel_path = os.path.relpath(file_path, directory_path)
            print(f"\n[{i}/{len(file_paths)}] å¤„ç†æ–‡ä»¶: {rel_path}")
            try:
                result = self._process_single_file(file_path)
                if result and self.move_processed:
                    self._move_to_processed(file_path)
            except Exception as e:
                print(f"å¤„ç†æ–‡ä»¶å¤±è´¥: {str(e)}")
                continue
   
    def _process_single_file(self, file_path):
        """å¤„ç†å•ä¸ªæ–‡ä»¶"""
        # ä½¿ç”¨TextReaderè¯»å–å’Œé¢„å¤„ç†æ–‡æœ¬
        text = self.reader.read_file(file_path)
        text = self.reader.preprocess_text(text)
        content_hash = self.storage_manager.calculate_text_hash(text)
        
        # æ£€æŸ¥ç¼“å­˜
        cached_result = self.storage_manager.get_existing_analysis(content_hash)
        if cached_result:
            print("æ‰¾åˆ°ç¼“å­˜çš„åˆ†æç»“æœ")
            return cached_result, content_hash
        
        # æ–°åˆ†æ
        print("æ²¡æœ‰ç¼“å­˜ï¼Œè¿›è¡Œåˆ†æ")
        start_time = time.time()  # å¼€å§‹è®¡æ—¶
        
        basic_info, word_frequencies = analyze_text(text, self.reader)
        
        # è®¡ç®—å¤„ç†æ—¶é•¿ï¼ˆç§’ï¼‰
        process_duration = time.time() - start_time
        
        # è·å–æ–‡ä»¶å…ƒæ•°æ®
        metadata = self.reader.get_metadata()
        basic_info.update(metadata)
        basic_info['filename'] = Path(file_path).name
        basic_info['analysis_date'] = datetime.now().isoformat()
        basic_info['process_duration'] = process_duration  # æ·»åŠ å¤„ç†æ—¶é•¿åˆ°åŸºæœ¬ä¿¡æ¯ä¸­
        
        self.storage_manager.store_analysis(
            content_hash=content_hash,
            filename=basic_info['filename'],
            basic_info=basic_info,
            word_frequencies=word_frequencies,
            process_duration=process_duration,
            original_text=text
        )
        
        # ç”Ÿæˆåˆ†ææŠ¥å‘Š
        self.save_analysis_report(file_path, basic_info, word_frequencies, content_hash)
        
        print(f"åˆ†æå®Œæˆå¹¶ä¿å­˜åˆ°æ•°æ®åº“ï¼Œå¤„ç†æ—¶é•¿ï¼š{process_duration:.4f}ç§’")
        return (basic_info, word_frequencies), content_hash

    def _move_to_processed(self, file_path):
        """å°†å¤„ç†å®Œçš„æ–‡ä»¶ç§»åŠ¨åˆ°processedç›®å½•"""
        try:
            file_path = Path(file_path)
            
            # ç¡®å®šprocessedç›®å½•è·¯å¾„
            # å¦‚æœæ–‡ä»¶åœ¨data/files/new/ä¸‹ï¼Œç§»åŠ¨åˆ°data/files/processed/
            if 'data/files/new' in str(file_path):
                processed_dir = Path('data/files/processed')
            else:
                # å¦åˆ™åœ¨åŒçº§ç›®å½•åˆ›å»ºprocessedç›®å½•
                processed_dir = file_path.parent / 'processed'
            
            # åˆ›å»ºprocessedç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            processed_dir.mkdir(parents=True, exist_ok=True)
            
            # ç›®æ ‡æ–‡ä»¶è·¯å¾„
            target_path = processed_dir / file_path.name
            
            # å¦‚æœç›®æ ‡æ–‡ä»¶å·²å­˜åœ¨ï¼Œæ·»åŠ æ—¶é—´æˆ³é¿å…å†²çª
            if target_path.exists():
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                stem = target_path.stem
                suffix = target_path.suffix
                target_path = processed_dir / f"{stem}_{timestamp}{suffix}"
            
            # ç§»åŠ¨æ–‡ä»¶
            shutil.move(str(file_path), str(target_path))
            print(f"ğŸ“ æ–‡ä»¶å·²ç§»åŠ¨: {file_path.name} â†’ {processed_dir.name}/")
            
        except Exception as e:
            print(f"âš ï¸  æ–‡ä»¶ç§»åŠ¨å¤±è´¥ {file_path}: {e}")
            # ç§»åŠ¨å¤±è´¥ä¸å½±å“ä¸»æµç¨‹ï¼Œç»§ç»­å¤„ç†å…¶ä»–æ–‡ä»¶

    def organize_existing_files(self):
        """æ•´ç†å·²å¤„ç†çš„æ–‡ä»¶ï¼šå°†æ•°æ®åº“ä¸­å·²å­˜åœ¨çš„æ–‡ä»¶ç§»åŠ¨åˆ°processedç›®å½•"""
        try:
            print("ğŸ—‚ï¸  å¼€å§‹æ•´ç†å·²å¤„ç†çš„æ–‡ä»¶...")
            
            # è·å–æ•°æ®åº“ä¸­æ‰€æœ‰å·²åˆ†æçš„æ–‡ä»¶å
            analyses = self.storage_manager.get_all_analyses()
            
            # å¤„ç†ä¸åŒçš„æ•°æ®æ ¼å¼
            if not analyses:
                print("ğŸ“Š æ•°æ®åº“ä¸­æ²¡æœ‰å·²åˆ†ææ–‡ä»¶")
                return
                
            # æå–æ–‡ä»¶åï¼Œå¤„ç†å…ƒç»„æ ¼å¼çš„æ•°æ®
            analyzed_filenames = set()
            for analysis in analyses:
                if isinstance(analysis, dict):
                    # å­—å…¸æ ¼å¼
                    analyzed_filenames.add(analysis['filename'])
                elif isinstance(analysis, (list, tuple)) and len(analysis) > 1:
                    # å…ƒç»„/åˆ—è¡¨æ ¼å¼ï¼Œå‡è®¾filenameåœ¨ç¬¬äºŒä¸ªä½ç½®
                    analyzed_filenames.add(analysis[1])
                else:
                    # å…¶ä»–æ ¼å¼ï¼Œå°è¯•ç›´æ¥æ·»åŠ 
                    try:
                        analyzed_filenames.add(str(analysis))
                    except:
                        continue
            
            print(f"ğŸ“Š æ•°æ®åº“ä¸­æœ‰ {len(analyzed_filenames)} ä¸ªå·²åˆ†ææ–‡ä»¶")
            
            # æ£€æŸ¥newç›®å½•ä¸­çš„æ–‡ä»¶
            new_dir = Path('data/files/new')
            if not new_dir.exists():
                print("âŒ data/files/new ç›®å½•ä¸å­˜åœ¨")
                return
            
            moved_count = 0
            for file_path in new_dir.rglob('*'):
                if file_path.is_file() and file_path.name in analyzed_filenames:
                    self._move_to_processed(file_path)
                    moved_count += 1
            
            print(f"âœ… æ•´ç†å®Œæˆï¼Œå…±ç§»åŠ¨äº† {moved_count} ä¸ªå·²å¤„ç†çš„æ–‡ä»¶")
            
        except Exception as e:
            print(f"âŒ æ•´ç†æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()

    def save_analysis_report(self, filepath: str, basic_info: dict, word_frequencies: dict, content_hash: str, generate_report: bool = True):
        """ä¿å­˜æ–‡æœ¬åˆ†ææŠ¥å‘Šåˆ°æ–‡ä»¶"""
        if not generate_report:
            return
            
        try:
            # ç¡®ä¿exportsç›®å½•å­˜åœ¨
            exports_dir = Path("data/exports")
            exports_dir.mkdir(parents=True, exist_ok=True)
            
            # ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶åï¼šæ–‡ä»¶å_åˆ†ææŠ¥å‘Š_æ—¶é—´æˆ³.txt
            filename = Path(filepath).stem
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_filename = f"{filename}_analysis_report_{timestamp}.txt"
            report_file = exports_dir / report_filename
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            total_words = basic_info.get('total_words', 0)
            unique_words = basic_info.get('unique_words', 0)
            total_sentences = basic_info.get('sentences', 0)
            total_paragraphs = basic_info.get('paragraphs', 0)
            file_size = os.path.getsize(filepath) if os.path.exists(filepath) else 0
            
            # è¯é¢‘åˆ†æ
            sorted_words = sorted(word_frequencies.items(), key=lambda x: x[1], reverse=True)
            most_frequent = sorted_words[:20]  # å‰20ä¸ªé«˜é¢‘è¯
            
            # è¯é•¿åˆ†æ
            word_lengths = [len(word) for word in word_frequencies.keys()]
            avg_word_length = sum(word_lengths) / len(word_lengths) if word_lengths else 0
            length_counter = Counter(word_lengths)
            
            # é¢‘ç‡åˆ†æ
            freq_values = list(word_frequencies.values())
            freq_counter = Counter(freq_values)
            single_occurrence = freq_counter.get(1, 0)
            
            with open(report_file, 'w', encoding='utf-8') as f:
                # æŠ¥å‘Šå¤´éƒ¨
                f.write("=" * 60 + "\n")
                f.write("          æ–‡æœ¬åˆ†ææŠ¥å‘Š\n")
                f.write("=" * 60 + "\n")
                f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"æºæ–‡ä»¶: {filepath}\n")
                f.write(f"æ–‡ä»¶å: {basic_info.get('filename', 'Unknown')}\n")
                f.write(f"æ–‡ä»¶å¤§å°: {file_size} bytes\n")
                f.write(f"å†…å®¹å“ˆå¸Œ: {content_hash[:16]}...\n")
                f.write(f"åˆ†ææ—¶é—´: {basic_info.get('analysis_date', 'Unknown')}\n")
                f.write(f"å¤„ç†æ—¶é•¿: {basic_info.get('process_duration', 0):.4f} ç§’\n")
                f.write("-" * 60 + "\n\n")
                
                # åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
                f.write("ğŸ“Š åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯\n")
                f.write("-" * 30 + "\n")
                f.write(f"æ€»è¯æ•°: {total_words}\n")
                f.write(f"ç‹¬ç‰¹è¯æ±‡æ•°: {unique_words}\n")
                f.write(f"å¥å­æ•°: {total_sentences}\n")
                f.write(f"æ®µè½æ•°: {total_paragraphs}\n")
                f.write(f"è¯æ±‡å¤ç”¨ç‡: {((total_words - unique_words) / total_words * 100):.1f}%\n")
                f.write(f"å¹³å‡è¯é•¿: {avg_word_length:.1f} å­—ç¬¦\n")
                if total_sentences > 0:
                    f.write(f"å¹³å‡å¥é•¿: {total_words / total_sentences:.1f} è¯\n")
                f.write("\n")
                
                # è¯é¢‘åˆ†æ
                f.write("ğŸ”¢ è¯é¢‘åˆ†æ\n")
                f.write("-" * 30 + "\n")
                f.write(f"å•æ¬¡å‡ºç°è¯æ±‡: {single_occurrence} ä¸ª ({(single_occurrence/unique_words*100):.1f}%)\n")
                f.write(f"æœ€é«˜é¢‘ç‡: {max(freq_values) if freq_values else 0}\n")
                f.write(f"å¹³å‡é¢‘ç‡: {sum(freq_values)/len(freq_values):.2f}\n")
                f.write("\n")
                
                # é«˜é¢‘è¯æ±‡
                if most_frequent:
                    f.write("ğŸ“ˆ é«˜é¢‘è¯æ±‡ (Top 20)\n")
                    f.write("-" * 30 + "\n")
                    for i, (word, freq) in enumerate(most_frequent, 1):
                        percentage = (freq / total_words) * 100
                        f.write(f"{i:2d}. {word:<15} {freq:>4} æ¬¡ ({percentage:.2f}%)\n")
                    f.write("\n")
                
                # è¯é•¿åˆ†å¸ƒ
                f.write("ğŸ“ è¯é•¿åˆ†å¸ƒ\n")
                f.write("-" * 30 + "\n")
                for length in sorted(length_counter.keys())[:15]:  # æ˜¾ç¤ºå‰15ç§è¯é•¿
                    count = length_counter[length]
                    percentage = (count / unique_words) * 100
                    f.write(f"{length:2d} å­—ç¬¦: {count:>4} è¯ ({percentage:.1f}%)\n")
                f.write("\n")
                
                # é¢‘ç‡åˆ†å¸ƒ
                f.write("ğŸ“Š é¢‘ç‡åˆ†å¸ƒ\n")
                f.write("-" * 30 + "\n")
                freq_ranges = [(1, 1), (2, 5), (6, 10), (11, 20), (21, 50), (51, float('inf'))]
                for min_freq, max_freq in freq_ranges:
                    if max_freq == float('inf'):
                        count = sum(1 for f in freq_values if f >= min_freq)
                        label = f"{min_freq}+ æ¬¡"
                    else:
                        count = sum(1 for f in freq_values if min_freq <= f <= max_freq)
                        if min_freq == max_freq:
                            label = f"{min_freq} æ¬¡"
                        else:
                            label = f"{min_freq}-{max_freq} æ¬¡"
                    
                    percentage = (count / unique_words) * 100 if unique_words > 0 else 0
                    f.write(f"{label:<10}: {count:>4} è¯ ({percentage:.1f}%)\n")
                f.write("\n")
                
                # è¯æ±‡è¡¨åŒ¹é…åˆ†æ
                f.write("ğŸ“š è¯æ±‡è¡¨åŒ¹é…åˆ†æ\n")
                f.write("-" * 30 + "\n")
                # è·å–æ‰€æœ‰è¯æ±‡è¡¨
                try:
                    wordlists = unified_adapter.get_all_wordlists()
                    if wordlists:
                        text_words = set(word_frequencies.keys())
                        for wl in wordlists:
                            wordlist_name = wl['name']
                            
                            # è·å–è¯æ±‡è¡¨ä¸­çš„æ‰€æœ‰è¯æ±‡
                            wordlist_words = unified_adapter.get_wordlist_words(wordlist_name)
                            wordlist_word_set = set(word.lower() for word in wordlist_words)
                            
                            # è®¡ç®—åŒ¹é…
                            matched_words = text_words.intersection(wordlist_word_set)
                            match_count = len(matched_words)
                            coverage_percentage = (match_count / unique_words * 100) if unique_words > 0 else 0
                            
                            # è®¡ç®—åŒ¹é…è¯æ±‡çš„æ€»é¢‘ç‡
                            matched_frequency = sum(word_frequencies[word] for word in matched_words)
                            frequency_percentage = (matched_frequency / total_words * 100) if total_words > 0 else 0
                            
                            f.write(f"ğŸ“– {wordlist_name}:\n")
                            f.write(f"   åŒ¹é…è¯æ±‡: {match_count}/{wl['word_count']} ä¸ª\n")
                            f.write(f"   è¦†ç›–ç‡: {coverage_percentage:.1f}% (æ–‡æœ¬ç‹¬ç‰¹è¯æ±‡)\n")
                            f.write(f"   é¢‘ç‡è¦†ç›–: {frequency_percentage:.1f}% (æ€»è¯é¢‘)\n")
                            if match_count > 0:
                                # æ˜¾ç¤ºå‰10ä¸ªåŒ¹é…çš„é«˜é¢‘è¯
                                matched_sorted = sorted(matched_words, key=lambda x: word_frequencies[x], reverse=True)
                                sample_words = matched_sorted[:10]
                                f.write(f"   åŒ¹é…ç¤ºä¾‹: {', '.join(sample_words)}\n")
                            f.write("\n")
                    else:
                        f.write("æš‚æ— åŠ è½½çš„è¯æ±‡è¡¨\n")
                except Exception as e:
                    f.write(f"è·å–è¯æ±‡è¡¨ä¿¡æ¯å¤±è´¥: {e}\n")
                f.write("\n")
                
                # å•æ¬¡å‡ºç°è¯æ±‡
                f.write("ğŸ” å•æ¬¡å‡ºç°è¯æ±‡åˆ—ä¸¾\n")
                f.write("-" * 30 + "\n")
                
                # è·å–åªå‡ºç°ä¸€æ¬¡çš„è¯æ±‡
                single_words = [word for word, freq in word_frequencies.items() if freq == 1]
                
                if single_words:
                    f.write(f"å…±æœ‰ {len(single_words)} ä¸ªè¯æ±‡åªå‡ºç°è¿‡ä¸€æ¬¡ï¼Œå ç‹¬ç‰¹è¯æ±‡çš„ {len(single_words)/unique_words*100:.1f}%\n\n")
                    
                    # æŒ‰å­—æ¯é¡ºåºæ’åˆ—æ˜¾ç¤º
                    sorted_single_words = sorted(single_words)
                    
                    # åˆ†é¡µæ˜¾ç¤ºï¼Œæ¯è¡Œ10ä¸ªè¯ï¼Œæœ€å¤šæ˜¾ç¤º100ä¸ª
                    display_count = min(len(sorted_single_words), 100)
                    for i in range(0, display_count, 10):
                        line_words = sorted_single_words[i:i+10]
                        f.write(f"  {', '.join(line_words)}\n")
                    
                    if len(sorted_single_words) > 100:
                        f.write(f"  ... è¿˜æœ‰ {len(sorted_single_words) - 100} ä¸ªå•æ¬¡å‡ºç°è¯æ±‡\n")
                    f.write("\n")
                    
                    # å•æ¬¡å‡ºç°è¯æ±‡çš„è¯é•¿åˆ†æ
                    single_word_lengths = [len(word) for word in single_words]
                    if single_word_lengths:
                        avg_single_length = sum(single_word_lengths) / len(single_word_lengths)
                        max_single_length = max(single_word_lengths)
                        min_single_length = min(single_word_lengths)
                        f.write(f"å•æ¬¡è¯æ±‡ç»Ÿè®¡: å¹³å‡é•¿åº¦ {avg_single_length:.1f} å­—ç¬¦ï¼Œæœ€é•¿ {max_single_length} å­—ç¬¦ï¼Œæœ€çŸ­ {min_single_length} å­—ç¬¦\n")
                        
                        # æ˜¾ç¤ºæœ€é•¿çš„å•æ¬¡å‡ºç°è¯æ±‡
                        longest_singles = [word for word in single_words if len(word) == max_single_length]
                        if longest_singles:
                            f.write(f"æœ€é•¿å•æ¬¡è¯æ±‡: {', '.join(longest_singles[:5])}\n")
                else:
                    f.write("æ²¡æœ‰åªå‡ºç°ä¸€æ¬¡çš„è¯æ±‡\n")
                f.write("\n")
                
                # æŠ¥å‘Šå°¾éƒ¨
                f.write("=" * 60 + "\n")
                f.write("åˆ†ææŠ¥å‘Šç”Ÿæˆå®Œæˆ\n")
                f.write("=" * 60 + "\n")
            
            print(f"ğŸ“„ åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
            return str(report_file)
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆåˆ†ææŠ¥å‘Šå¤±è´¥: {e}")
            return None

# å‘åå…¼å®¹åˆ«å
FileProcessor = TextProcessor
