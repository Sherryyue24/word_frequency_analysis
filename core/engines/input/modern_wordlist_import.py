# ç°ä»£åŒ–è¯æ±‡è¡¨å¯¼å…¥æ¨¡å—
# è·¯å¾„: core/engines/input/modern_wordlist_import.py
# é¡¹ç›®åç§°: Word Frequency Analysis
# ä½œè€…: Sherryyue

from pathlib import Path
from datetime import datetime
from ..database.database_adapter import unified_adapter
from typing import List, Tuple, Dict

def save_import_report(output_file: str, filepath: str, tag_name: str, stats: dict, skipped_lines: dict, processing_details: dict):
    """ä¿å­˜å¯¼å…¥æŠ¥å‘Šåˆ°æ–‡ä»¶"""
    with open(output_file, 'w', encoding='utf-8') as f:
        # æŠ¥å‘Šå¤´éƒ¨
        f.write("=" * 60 + "\n")
        f.write("          è¯æ±‡è¡¨å¯¼å…¥æŠ¥å‘Š\n")
        f.write("=" * 60 + "\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"æºæ–‡ä»¶: {filepath}\n")
        f.write(f"è¯æ±‡è¡¨æ ‡ç­¾: {tag_name}\n")
        f.write(f"æ–‡ä»¶å¤§å°: {stats['file_size']} bytes\n")
        f.write(f"æ€»å­—ç¬¦æ•°: {stats['char_count']}\n")
        f.write(f"æ€»è¡Œæ•°: {stats['total_lines']}\n")
        f.write("-" * 60 + "\n\n")
        
        # å¤„ç†ç»“æœæ¦‚è§ˆ
        f.write("ğŸ“Š å¤„ç†ç»“æœæ¦‚è§ˆ\n")
        f.write("-" * 30 + "\n")
        f.write(f"æœ‰æ•ˆè¯æ±‡æ•°: {stats['valid_words_count']}\n")
        f.write(f"æˆåŠŸå¯¼å…¥: {stats['imported_count']}\n")
        f.write(f"å·²å­˜åœ¨è¯æ±‡: {stats['existing_count']}\n")
        f.write(f"æ€»è·³è¿‡è¡Œæ•°: {stats['total_skipped']}\n")
        f.write(f"å¯¼å…¥æˆåŠŸç‡: {stats['success_rate']:.1f}%\n")
        f.write(f"è¯æ±‡æå–ç‡: {stats['extraction_rate']:.1f}%\n\n")
        
        # è·³è¿‡ç»Ÿè®¡è¯¦æƒ…
        f.write("ğŸš« è·³è¿‡ç»Ÿè®¡è¯¦æƒ…\n")
        f.write("-" * 30 + "\n")
        f.write(f"ç©ºè¡Œæ•°é‡: {len(skipped_lines['empty'])}\n")
        f.write(f"æ ‡é¢˜è¡Œæ•°é‡: {len(skipped_lines['title'])}\n")
        f.write(f"æ— æ•ˆè¯æ±‡æ•°é‡: {len(skipped_lines['invalid'])}\n")
        f.write(f"æ¸…ç†å¤±è´¥æ•°é‡: {len(skipped_lines['clean_failed'])}\n")
        f.write(f"æ ¼å¼é”™è¯¯æ•°é‡: {len(skipped_lines['format_error'])}\n")
        f.write(f"å·²å­˜åœ¨è¯æ±‡æ•°é‡: {len(skipped_lines['already_exists'])}\n")
        f.write(f"å¯¼å…¥å¤±è´¥æ•°é‡: {len(skipped_lines['import_failed'])}\n\n")
        
        # å¤„ç†ç¤ºä¾‹
        f.write("ğŸ” å¤„ç†ç¤ºä¾‹\n")
        f.write("-" * 30 + "\n")
        if processing_details['cleaned_examples']:
            f.write("è¯æ±‡æ¸…ç†ç¤ºä¾‹:\n")
            for original, cleaned in processing_details['cleaned_examples'][:10]:
                f.write(f"  '{original}' â†’ '{cleaned}'\n")
            f.write("\n")
        
        # è¯¦ç»†é”™è¯¯ä¿¡æ¯
        if skipped_lines['title']:
            f.write("ğŸ“‹ æ ‡é¢˜è¡Œè¯¦æƒ…:\n")
            for line_num, line in skipped_lines['title'][:20]:  # é™åˆ¶æ˜¾ç¤ºæ•°é‡
                f.write(f"  ç¬¬ {line_num} è¡Œ: {line}\n")
            f.write("\n")
        
        if skipped_lines['invalid']:
            f.write("âŒ æ— æ•ˆè¯æ±‡è¯¦æƒ…:\n")
            for line_num, original_word, reason in skipped_lines['invalid'][:20]:
                f.write(f"  ç¬¬ {line_num} è¡Œ: '{original_word}' - {reason}\n")
            f.write("\n")
        
        if skipped_lines['clean_failed']:
            f.write("ğŸ§¹ æ¸…ç†å¤±è´¥è¯¦æƒ…:\n")
            for line_num, original_word in skipped_lines['clean_failed'][:20]:
                f.write(f"  ç¬¬ {line_num} è¡Œ: '{original_word}'\n")
            f.write("\n")
        
        if skipped_lines['format_error']:
            f.write("âš ï¸  æ ¼å¼é”™è¯¯è¯¦æƒ…:\n")
            for line_num, line, error in skipped_lines['format_error'][:20]:
                f.write(f"  ç¬¬ {line_num} è¡Œ: {line}\n")
                f.write(f"    é”™è¯¯: {error}\n")
            f.write("\n")
        
        if skipped_lines['already_exists']:
            f.write("ğŸ”„ é‡å¤è¯æ±‡è¯¦æƒ…:\n")
            for line_num, word, reason in skipped_lines['already_exists'][:50]:
                f.write(f"  ç¬¬ {line_num} è¡Œ: '{word}' - {reason}\n")
            if len(skipped_lines['already_exists']) > 50:
                f.write(f"  ... è¿˜æœ‰ {len(skipped_lines['already_exists']) - 50} ä¸ªé‡å¤è¯æ±‡\n")
            f.write("\n")
        
        if skipped_lines['import_failed']:
            f.write("ğŸ’¥ å¯¼å…¥å¤±è´¥è¯¦æƒ…:\n")
            f.write(f"æ€»è®¡ {len(skipped_lines['import_failed'])} ä¸ªè¯æ±‡å¯¼å…¥å¤±è´¥\n\n")
            
            # æŒ‰é”™è¯¯ç±»å‹åˆ†ç»„æ˜¾ç¤º
            error_groups = {}
            for line_num, word, error in skipped_lines['import_failed']:
                if error not in error_groups:
                    error_groups[error] = []
                error_groups[error].append((line_num, word))
            
            for error_type, failed_items in error_groups.items():
                f.write(f"é”™è¯¯ç±»å‹: {error_type}\n")
                f.write(f"å½±å“è¯æ±‡æ•°: {len(failed_items)}\n")
                f.write("å¤±è´¥è¯æ±‡åˆ—è¡¨:\n")
                for line_num, word in failed_items[:50]:  # é™åˆ¶æ˜¾ç¤ºæ•°é‡é¿å…æŠ¥å‘Šè¿‡é•¿
                    f.write(f"  ç¬¬ {line_num} è¡Œ: '{word}'\n")
                if len(failed_items) > 50:
                    f.write(f"  ... è¿˜æœ‰ {len(failed_items) - 50} ä¸ªè¯æ±‡\n")
                f.write("\n")
            
            # å®Œæ•´å¤±è´¥è¯æ±‡æ¸…å•
            f.write("ğŸ“‹ å®Œæ•´å¤±è´¥è¯æ±‡æ¸…å•:\n")
            all_failed_words = [word for _, word, _ in skipped_lines['import_failed']]
            for i, word in enumerate(all_failed_words, 1):
                f.write(f"  {i:3d}. {word}\n")
                if i % 20 == 0 and i < len(all_failed_words):
                    f.write("      ...\n")
            f.write("\n")
        
        # æˆåŠŸå¯¼å…¥çš„è¯æ±‡æ ·æœ¬
        if processing_details['imported_examples']:
            f.write("âœ… æˆåŠŸå¯¼å…¥è¯æ±‡æ ·æœ¬:\n")
            for word in processing_details['imported_examples'][:30]:
                f.write(f"  - {word}\n")
            f.write("\n")
        
        # æŠ¥å‘Šå°¾éƒ¨
        f.write("=" * 60 + "\n")
        f.write("æŠ¥å‘Šç”Ÿæˆå®Œæˆ\n")
        f.write("=" * 60 + "\n")

def import_wordlist_from_file(filepath: str, tag_name: str = None, description: str = None, 
                            generate_report: bool = True) -> bool:
    """
    ä»æ–‡ä»¶å¯¼å…¥è¯æ±‡è¡¨åˆ°ç»Ÿä¸€æ•°æ®åº“ï¼ˆå¸¦è¯¦ç»†æŠ¥å‘Šï¼‰
    
    Args:
        filepath: è¯æ±‡è¡¨æ–‡ä»¶è·¯å¾„
        tag_name: è¯æ±‡è¡¨æ ‡ç­¾åï¼ˆå¦‚æœä¸æä¾›åˆ™ä½¿ç”¨æ–‡ä»¶åï¼‰
        description: è¯æ±‡è¡¨æè¿°
        generate_report: æ˜¯å¦ç”Ÿæˆå¯¼å…¥æŠ¥å‘Š
        
    Returns:
        bool: å¯¼å…¥æ˜¯å¦æˆåŠŸ
    """
    try:
        filepath = Path(filepath)
        
        # å¦‚æœæ²¡æœ‰æä¾›æ ‡ç­¾åï¼Œä½¿ç”¨æ–‡ä»¶å
        if not tag_name:
            tag_name = filepath.stem
        
        # åˆå§‹åŒ–ç»Ÿè®¡ä¿¡æ¯
        skipped_lines = {
            'empty': [],           # ç©ºè¡Œ
            'title': [],           # æ ‡é¢˜è¡Œ (line_num, line)
            'invalid': [],         # æ— æ•ˆè¯æ±‡ (line_num, original_word, reason)
            'clean_failed': [],    # æ¸…ç†å¤±è´¥ (line_num, original_word)
            'format_error': [],    # æ ¼å¼é”™è¯¯ (line_num, line, error)
            'already_exists': [],  # å·²å­˜åœ¨ (line_num, word)
            'import_failed': []    # å¯¼å…¥å¤±è´¥ (line_num, word, error)
        }
        
        processing_details = {
            'cleaned_examples': [],    # æ¸…ç†ç¤ºä¾‹ (original, cleaned)
            'imported_examples': []    # å¯¼å…¥æˆåŠŸçš„è¯æ±‡æ ·æœ¬
        }
        
        # è¯»å–æ–‡ä»¶åŸºæœ¬ä¿¡æ¯
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()
            file_size = len(content.encode('utf-8'))
            char_count = len(content)
        
        # å¤„ç†è¯æ±‡æ–‡ä»¶
        words_to_import = []
        total_lines = 0
        word_line_mapping = {}  # è¿½è¸ªè¯æ±‡é¦–æ¬¡å‡ºç°çš„è¡Œå·
        
        with open(filepath, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                total_lines += 1
                original_line = line
                line = line.strip()
                
                # ç©ºè¡Œå¤„ç†
                if not line:
                    skipped_lines['empty'].append(line_num)
                    continue
                
                try:
                    # æ”¯æŒå¤šç§æ ¼å¼å¤„ç†
                    # 1. åˆ¶è¡¨ç¬¦åˆ†éš”æ ¼å¼ï¼ˆå¦‚AWLï¼‰ï¼šå–ç¬¬ä¸€åˆ—
                    if '\t' in line:
                        word = line.split('\t')[0].strip()
                    # 2. é€—å·åˆ†éš”æ ¼å¼ï¼šå¤„ç†æ‰€æœ‰è¯æ±‡
                    elif ',' in line and '\t' not in line:
                        line_words = [w.strip() for w in line.split(',') if w.strip()]
                        for word in line_words:
                            processed = _process_single_word(word, line_num, skipped_lines, processing_details)
                            if processed:
                                # æ£€æŸ¥é‡å¤è¯æ±‡
                                if processed in word_line_mapping:
                                    skipped_lines['already_exists'].append((line_num, processed, f"é‡å¤è¯æ±‡ï¼Œé¦–æ¬¡å‡ºç°åœ¨ç¬¬{word_line_mapping[processed]}è¡Œ"))
                                else:
                                    word_line_mapping[processed] = line_num
                                    words_to_import.append((processed, line_num))
                        continue
                    # 3. æ¯è¡Œä¸€ä¸ªè¯æ ¼å¼
                    else:
                        word = line.split()[0] if line.split() else ''
                    
                    if not word:
                        skipped_lines['empty'].append(line_num)
                        continue
                    
                    processed = _process_single_word(word, line_num, skipped_lines, processing_details)
                    if processed:
                        # æ£€æŸ¥é‡å¤è¯æ±‡
                        if processed in word_line_mapping:
                            skipped_lines['already_exists'].append((line_num, processed, f"é‡å¤è¯æ±‡ï¼Œé¦–æ¬¡å‡ºç°åœ¨ç¬¬{word_line_mapping[processed]}è¡Œ"))
                        else:
                            word_line_mapping[processed] = line_num
                            words_to_import.append((processed, line_num))
                            
                except Exception as e:
                    skipped_lines['format_error'].append((line_num, original_line.strip(), str(e)))
                    continue
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        valid_words_count = len(words_to_import)
        total_skipped = sum(len(v) for v in skipped_lines.values())
        
        if not words_to_import:
            print(f"âŒ æ–‡ä»¶ {filepath} ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆè¯æ±‡")
            return False
        
        print(f"ğŸ“š ä» {filepath.name} è¯»å–åˆ° {valid_words_count} ä¸ªæœ‰æ•ˆè¯æ±‡")
        print(f"ğŸ“Š è·³è¿‡ {total_skipped} è¡Œæ— æ•ˆå†…å®¹")
        
        # å¯¼å…¥åˆ°æ•°æ®åº“ - æ”¹è¿›ç‰ˆï¼šè·Ÿè¸ªæ¯ä¸ªè¯æ±‡çš„å¯¼å…¥çŠ¶æ€
        words_only = [word for word, _ in words_to_import]
        import_result, failed_words = _import_words_with_tracking(tag_name, words_to_import)
        
        # ç»Ÿè®¡å¯¼å…¥ç»“æœ
        success = import_result.get('success', False)
        imported_count = import_result.get('new_associations', 0)
        existing_count = import_result.get('existing_associations', 0)
        
        # è®°å½•å¤±è´¥çš„è¯æ±‡
        for failed_word, line_num, error in failed_words:
            skipped_lines['import_failed'].append((line_num, failed_word, error))
        
        # æ”¶é›†å¯¼å…¥æˆåŠŸçš„è¯æ±‡æ ·æœ¬
        if success and imported_count > 0:
            processing_details['imported_examples'] = [word for word, _ in words_to_import 
                                                     if word not in [fw[0] for fw in failed_words]][:30]
        
        # è®¡ç®—æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
        stats = {
            'file_size': file_size,
            'char_count': char_count,
            'total_lines': total_lines,
            'valid_words_count': valid_words_count,
            'imported_count': imported_count,
            'existing_count': existing_count,
            'total_skipped': total_skipped,
            'success_rate': (imported_count / valid_words_count * 100) if valid_words_count > 0 else 0,
            'extraction_rate': (valid_words_count / total_lines * 100) if total_lines > 0 else 0
        }
        
        # ç”Ÿæˆå¯¼å…¥æŠ¥å‘Š
        if generate_report:
            # ç¡®ä¿exportsç›®å½•å­˜åœ¨
            exports_dir = Path("data/exports")
            exports_dir.mkdir(parents=True, exist_ok=True)
            
            # ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶åï¼šè¯æ±‡è¡¨å_å¯¼å…¥æŠ¥å‘Š_æ—¶é—´æˆ³.txt
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_filename = f"{tag_name}_import_report_{timestamp}.txt"
            report_file = exports_dir / report_filename
            
            save_import_report(str(report_file), str(filepath), tag_name, stats, skipped_lines, processing_details)
            print(f"ğŸ“„ å¯¼å…¥æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        
        if success:
            print(f"âœ… æˆåŠŸå¯¼å…¥è¯æ±‡è¡¨ '{tag_name}' ({imported_count} ä¸ªè¯æ±‡)")
            print(f"ğŸ“Š æå–ç‡: {stats['extraction_rate']:.1f}%, æˆåŠŸç‡: {stats['success_rate']:.1f}%")
            return True
        else:
            print(f"âŒ å¯¼å…¥è¯æ±‡è¡¨ '{tag_name}' å¤±è´¥")
            return False
            
    except FileNotFoundError:
        print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {filepath}")
        return False
    except Exception as e:
        print(f"âŒ å¯¼å…¥è¿‡ç¨‹å‡ºé”™: {e}")
        return False

def _process_single_word(word: str, line_num: int, skipped_lines: dict, processing_details: dict) -> str:
    """å¤„ç†å•ä¸ªè¯æ±‡å¹¶æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
    original_word = word
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯æ ‡é¢˜è¡Œ
    if any(phrase in word.lower() for phrase in ['word', 'list', 'part', 'unit', 'lesson']):
        if word.lower().startswith('word') or word.isdigit():
            skipped_lines['title'].append((line_num, word))
            return ''
    
    # æ¸…ç†è¯æ±‡
    clean_word = _clean_vocabulary_word(word)
    
    if not clean_word:
        skipped_lines['clean_failed'].append((line_num, original_word))
        return ''
    
    if len(clean_word) <= 1:
        skipped_lines['invalid'].append((line_num, original_word, "è¯æ±‡å¤ªçŸ­"))
        return ''
    
    # è®°å½•æ¸…ç†ç¤ºä¾‹
    if original_word != clean_word and len(processing_details['cleaned_examples']) < 20:
        processing_details['cleaned_examples'].append((original_word, clean_word))
    
    return clean_word.lower()

def import_multiple_wordlists(directory: str, pattern: str = "*.txt", generate_reports: bool = True) -> int:
    """
    æ‰¹é‡å¯¼å…¥ç›®å½•ä¸‹çš„è¯æ±‡è¡¨æ–‡ä»¶
    
    Args:
        directory: è¯æ±‡è¡¨æ–‡ä»¶ç›®å½•
        pattern: æ–‡ä»¶åæ¨¡å¼
        generate_reports: æ˜¯å¦ä¸ºæ¯ä¸ªæ–‡ä»¶ç”Ÿæˆå¯¼å…¥æŠ¥å‘Š
        
    Returns:
        int: æˆåŠŸå¯¼å…¥çš„è¯æ±‡è¡¨æ•°é‡
    """
    directory = Path(directory)
    
    if not directory.exists():
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {directory}")
        return 0
    
    wordlist_files = list(directory.glob(pattern))
    
    if not wordlist_files:
        print(f"âŒ ç›®å½• {directory} ä¸­æœªæ‰¾åˆ°åŒ¹é… {pattern} çš„æ–‡ä»¶")
        return 0
    
    print(f"ğŸ” æ‰¾åˆ° {len(wordlist_files)} ä¸ªè¯æ±‡è¡¨æ–‡ä»¶")
    
    success_count = 0
    total_reports = []
    
    for file_path in wordlist_files:
        print(f"\nå¤„ç†æ–‡ä»¶: {file_path.name}")
        if import_wordlist_from_file(str(file_path), generate_report=generate_reports):
            success_count += 1
            if generate_reports:
                # è®°å½•æŠ¥å‘Šæ–‡ä»¶ï¼ˆåœ¨exportsç›®å½•ä¸­ï¼‰
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                report_name = f"{file_path.stem}_import_report_{timestamp}.txt"
                total_reports.append(f"data/exports/{report_name}")
    
    print(f"\nğŸ“Š æ‰¹é‡å¯¼å…¥å®Œæˆ: {success_count}/{len(wordlist_files)} ä¸ªè¯æ±‡è¡¨å¯¼å…¥æˆåŠŸ")
    if generate_reports and total_reports:
        print(f"ğŸ“„ ç”Ÿæˆäº† {len(total_reports)} ä¸ªå¯¼å…¥æŠ¥å‘Šï¼Œä¿å­˜åœ¨ data/exports/ ç›®å½•")
    
    return success_count

def get_available_wordlists() -> list:
    """è·å–å¯ç”¨çš„è¯æ±‡è¡¨åˆ—è¡¨"""
    return unified_adapter.get_all_wordlists()

def get_wordlist_stats() -> dict:
    """è·å–è¯æ±‡è¡¨ç»Ÿè®¡ä¿¡æ¯"""
    return unified_adapter.get_database_info()

# å‘åå…¼å®¹çš„å‡½æ•°å
def import_wordlist_to_database(db, words, tag_name="Imported"):
    """å‘åå…¼å®¹çš„å¯¼å…¥å‡½æ•°"""
    print(f"âš ï¸  ä½¿ç”¨å‘åå…¼å®¹æ¨¡å¼å¯¼å…¥ {len(words)} ä¸ªè¯æ±‡åˆ°æ ‡ç­¾ '{tag_name}'")
    return unified_adapter.add_words_to_wordlist(tag_name, words)

def _clean_vocabulary_word(word: str) -> str:
    """æ¸…ç†è¯æ±‡è¡¨ä¸­çš„è¯æ±‡ï¼Œç§»é™¤æ ‡è®°ç¬¦å·ä½†ä¿ç•™æœ‰æ•ˆå†…å®¹"""
    
    # è·³è¿‡æ˜æ˜¾çš„æ ‡é¢˜è¡Œ
    if any(phrase in word.lower() for phrase in ['word', 'list', 'part', 'unit', 'lesson']):
        if word.lower().startswith('word') or word.isdigit():
            return ''
    
    # åŸºæœ¬æ¸…ç†ï¼šç§»é™¤å‰åç©ºæ ¼
    clean_word = word.strip()
    
    # è·³è¿‡çº¯æ•°å­—è¯æ±‡
    if clean_word.isdigit():
        return ''
    
    # è·³è¿‡åŒ…å«æ•°å­—çš„è¯æ±‡ï¼ˆé™¤äº†ç‰¹å®šå¹´ä»£æ ¼å¼å¦‚1920sï¼‰
    if any(c.isdigit() for c in clean_word):
        # å…è®¸å¹´ä»£æ ¼å¼ï¼ˆå¦‚1920s, 1990sï¼‰
        if not (clean_word.endswith('s') and clean_word[:-1].isdigit() and len(clean_word) >= 5):
            return ''
    
    # ç§»é™¤å¸¸è§çš„æ ‡è®°ç¬¦å·
    clean_word = clean_word.rstrip('*')  # ç§»é™¤å°¾éƒ¨æ˜Ÿå·
    clean_word = clean_word.strip('[](){}')  # ç§»é™¤æ‹¬å·
    clean_word = clean_word.strip('"\'')  # ç§»é™¤å¼•å·
    
    # è·³è¿‡è¿‡çŸ­çš„è¯æ±‡ï¼ˆå°‘äº2ä¸ªå­—ç¬¦ï¼Œé™¤äº†"I"å’Œ"a"ï¼‰
    if len(clean_word) < 2 and clean_word.lower() not in ['i', 'a']:
        return ''
    
    # è·³è¿‡è¿‡é•¿çš„è¯æ±‡ï¼ˆè¶…è¿‡30ä¸ªå­—ç¬¦ï¼Œå¯èƒ½æ˜¯é”™è¯¯æ•°æ®ï¼‰
    if len(clean_word) > 30:
        return ''
    
    # å¤„ç†è¿å­—ç¬¦è¯æ±‡ - ä¿å®ˆå¤„ç†ï¼Œä¿ç•™å®Œæ•´è¯æ±‡
    if '-' in clean_word:
        # ç§»é™¤æ˜æ˜¾çš„å‰ç¼€æ ‡è®°ï¼ˆå¦‚ç¼–å·æˆ–æ ¼å¼æ ‡è®°ï¼‰
        if clean_word.startswith('-') or clean_word.endswith('-'):
            clean_word = clean_word.strip('-')
        # è·³è¿‡çœ‹èµ·æ¥åƒç¼–å·çš„è¯æ±‡ï¼ˆå¦‚ "1-2", "a-z"ï¼‰
        if len(clean_word.split('-')) == 2:
            parts = clean_word.split('-')
            if all(len(part) <= 2 for part in parts):
                return ''
    
    # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆè¯æ±‡
    # å…è®¸å­—æ¯ã€è¿å­—ç¬¦ã€æ’‡å·ï¼ˆå¦‚don't, it'sï¼‰
    allowed_chars = set("abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ-'")
    if clean_word and len(clean_word) >= 2 and all(c in allowed_chars for c in clean_word):
        # ç¡®ä¿ä¸æ˜¯çº¯ç¬¦å·
        if any(c.isalpha() for c in clean_word):
            return clean_word
    
    return ''

def _import_words_with_tracking(tag_name: str, words_with_lines: List[Tuple[str, int]]) -> Tuple[Dict, List[Tuple[str, int, str]]]:
    """
    é€ä¸ªå¯¼å…¥è¯æ±‡å¹¶è·Ÿè¸ªå¤±è´¥æƒ…å†µ
    
    Args:
        tag_name: è¯æ±‡è¡¨æ ‡ç­¾å
        words_with_lines: è¯æ±‡åŠå…¶è¡Œå·çš„åˆ—è¡¨ [(word, line_num), ...]
        
    Returns:
        Tuple[Dict, List]: (å¯¼å…¥ç»“æœç»Ÿè®¡, å¤±è´¥è¯æ±‡åˆ—è¡¨ [(word, line_num, error), ...])
    """
    
    # åˆ›å»ºè¯æ±‡åˆ°è¡Œå·çš„æ˜ å°„
    word_to_line = {word: line_num for word, line_num in words_with_lines}
    
    # å°è¯•æ‰¹é‡å¯¼å…¥
    words_only = [word for word, _ in words_with_lines]
    try:
        import_result = unified_adapter.add_words_to_wordlist(tag_name, words_only)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰éªŒè¯å¤±è´¥çš„è¯æ±‡
        failed_words = []
        if 'invalid_words' in import_result:
            for word, error in import_result['invalid_words']:
                line_num = word_to_line.get(word, 0)
                failed_words.append((word, line_num, error))
        
        if import_result.get('success', False):
            # å¯¼å…¥æˆåŠŸï¼ˆå¯èƒ½æ˜¯éƒ¨åˆ†æˆåŠŸï¼‰
            return import_result, failed_words
        else:
            # æ‰¹é‡å¯¼å…¥å®Œå…¨å¤±è´¥ï¼Œé€ä¸ªéªŒè¯æ‰¾å‡ºé—®é¢˜è¯æ±‡
            return _fallback_individual_import(tag_name, words_with_lines, import_result.get('error', 'æ‰¹é‡å¯¼å…¥å¤±è´¥'))
            
    except Exception as e:
        # æ‰¹é‡å¯¼å…¥å¼‚å¸¸ï¼Œé€ä¸ªéªŒè¯
        return _fallback_individual_import(tag_name, words_with_lines, str(e))

def _fallback_individual_import(tag_name: str, words_with_lines: List[Tuple[str, int]], batch_error: str) -> Tuple[Dict, List[Tuple[str, int, str]]]:
    """
    å½“æ‰¹é‡å¯¼å…¥å¤±è´¥æ—¶ï¼Œé€ä¸ªå¯¼å…¥è¯æ±‡ä»¥ç¡®å®šå…·ä½“å¤±è´¥åŸå› 
    
    Args:
        tag_name: è¯æ±‡è¡¨æ ‡ç­¾å
        words_with_lines: è¯æ±‡åŠå…¶è¡Œå·çš„åˆ—è¡¨
        batch_error: æ‰¹é‡å¯¼å…¥çš„é”™è¯¯ä¿¡æ¯
        
    Returns:
        Tuple[Dict, List]: (å¯¼å…¥ç»“æœç»Ÿè®¡, å¤±è´¥è¯æ±‡åˆ—è¡¨)
    """
    
    successful_imports = 0
    existing_words = 0
    failed_words = []
    
    print(f"âš ï¸  æ‰¹é‡å¯¼å…¥å¤±è´¥ ({batch_error})ï¼Œæ­£åœ¨é€ä¸ªéªŒè¯è¯æ±‡...")
    
    for word, line_num in words_with_lines:
        try:
            # é€ä¸ªå¯¼å…¥è¯æ±‡
            individual_result = unified_adapter.add_words_to_wordlist(tag_name, [word])
            
            if individual_result.get('success', False):
                new_count = individual_result.get('new_associations', 0)
                existing_count = individual_result.get('existing_associations', 0)
                
                successful_imports += new_count
                existing_words += existing_count
            else:
                error_msg = individual_result.get('error', 'æœªçŸ¥é”™è¯¯')
                failed_words.append((word, line_num, error_msg))
                
        except Exception as e:
            failed_words.append((word, line_num, str(e)))
    
    # æ„å»ºç»“æœç»Ÿè®¡
    total_words = len(words_with_lines)
    result_stats = {
        'total_words': total_words,
        'new_associations': successful_imports,
        'existing_associations': existing_words,
        'failed_imports': len(failed_words),
        'success': len(failed_words) < total_words  # åªè¦æœ‰éƒ¨åˆ†æˆåŠŸå°±ç®—æˆåŠŸ
    }
    
    print(f"âœ… é€ä¸ªéªŒè¯å®Œæˆ: {successful_imports}ä¸ªæ–°å¢, {existing_words}ä¸ªå·²å­˜åœ¨, {len(failed_words)}ä¸ªå¤±è´¥")
    
    return result_stats, failed_words
