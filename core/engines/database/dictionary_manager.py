# å­—å…¸ç®¡ç†å™¨ - ç³»ç»Ÿå†…ç½®è¯å…¸ç®¡ç†
# è·¯å¾„: core/engines/database/dictionary_manager.py
# é¡¹ç›®åç§°: Word Frequency Analysis
# ä½œè€…: Sherryyue

import uuid
import csv
import json
import sqlite3
import re
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from datetime import datetime
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DictionaryManager:
    """ç³»ç»Ÿå†…ç½®å­—å…¸ç®¡ç†å™¨
    
    è´Ÿè´£ç®¡ç†ç³»ç»Ÿå†…ç½®çš„æƒå¨è¯å…¸æ•°æ®ï¼ŒåŒ…æ‹¬ï¼š
    - COCAè¯é¢‘è¡¨å¯¼å…¥å’Œç®¡ç†
    - WordNetå®šä¹‰æ•°æ®é›†æˆ
    - å­—å…¸æ•°æ®çš„æŸ¥è¯¢å’Œç»Ÿè®¡
    
    æ³¨æ„ï¼šè¿™ä¸æ˜¯ç”¨æˆ·è¾“å…¥åŠŸèƒ½ï¼Œè€Œæ˜¯ç³»ç»Ÿç»´æŠ¤åŠŸèƒ½
    """
    
    def __init__(self, db_path: str = "data/databases/unified.db"):
        self.db_path = db_path
        self.wordnet_available = False
        self._init_wordnet()
    
    def _init_wordnet(self):
        """åˆå§‹åŒ–WordNetèµ„æº"""
        try:
            import nltk
            from nltk.corpus import wordnet as wn
            self.wn = wn
            self.wordnet_available = True
            logger.info("âœ… WordNetå·²åŠ è½½ï¼Œå¯ç”¨äºè¡¥å……å®šä¹‰ä¿¡æ¯")
        except ImportError:
            logger.warning("âš ï¸ NLTK/WordNetä¸å¯ç”¨ï¼Œå°†è·³è¿‡å®šä¹‰è¡¥å……")
    
    def import_coca_dictionary(self, 
                              coca_file_path: str, 
                              max_words: Optional[int] = None,
                              skip_proper_nouns: bool = True) -> Dict[str, int]:
        """å¯¼å…¥COCAè¯é¢‘è¡¨ä½œä¸ºç³»ç»Ÿå†…ç½®è¯å…¸
        
        Args:
            coca_file_path: COCAè¯è¡¨æ–‡ä»¶è·¯å¾„
            max_words: æœ€å¤§å¯¼å…¥è¯æ±‡æ•°ï¼ˆNoneä¸ºå…¨éƒ¨ï¼‰
            skip_proper_nouns: æ˜¯å¦è·³è¿‡ä¸“æœ‰åè¯ï¼ˆé¦–å­—æ¯å¤§å†™ï¼‰
            
        Returns:
            å¯¼å…¥ç»Ÿè®¡ä¿¡æ¯
        """
        logger.info(f"ğŸš€ å¼€å§‹å¯¼å…¥COCAç³»ç»Ÿè¯å…¸: {coca_file_path}")
        
        stats = {
            'total_processed': 0,
            'successfully_imported': 0,
            'skipped_proper_nouns': 0,
            'wordnet_definitions_added': 0,
            'errors': 0
        }
        
        try:
            # è¯»å–COCAæ–‡ä»¶
            with open(coca_file_path, 'r', encoding='utf-8') as file:
                # æ£€æµ‹æ˜¯å¦æœ‰è¡¨å¤´
                first_line = file.readline().strip()
                file.seek(0)
                
                # å¦‚æœç¬¬ä¸€è¡ŒåŒ…å«"RANK"ç­‰å…³é”®è¯ï¼Œè·³è¿‡è¡¨å¤´
                has_header = any(keyword in first_line.upper() 
                               for keyword in ['RANK', 'POS', 'WORD', 'TOTAL'])
                
                reader = csv.reader(file)  # ä½¿ç”¨é»˜è®¤é€—å·åˆ†éš”ç¬¦
                if has_header:
                    next(reader)  # è·³è¿‡è¡¨å¤´
                
                # æ‰¹é‡å¤„ç†æ•°æ®
                batch_size = 1000
                batch_data = []
                
                for row in reader:
                    if len(row) < 4:  # è‡³å°‘éœ€è¦rank, pos, word_caps, word_lower
                        continue
                    
                    stats['total_processed'] += 1
                    
                    # é™åˆ¶å¯¼å…¥æ•°é‡
                    if max_words and stats['total_processed'] > max_words:
                        break
                    
                    # è§£ææ•°æ®
                    try:
                        rank = int(row[0])
                        pos = row[1].strip()
                        word_caps = row[2].strip()  # å¤§å†™å½¢å¼
                        word = row[3].strip()       # å°å†™å½¢å¼ï¼ˆå®é™…ä½¿ç”¨ï¼‰
                        
                        # è·³è¿‡æ— æ•ˆæˆ–ä¸“æœ‰åè¯
                        if not word or len(word) < 2:
                            continue
                        
                        # è·³è¿‡ä¸“æœ‰åè¯ï¼ˆæ ¹æ®éœ€è¦ï¼‰
                        if skip_proper_nouns and (word != word.lower() or word.isupper()):
                            stats['skipped_proper_nouns'] += 1
                            continue
                        
                        # æ ‡å‡†åŒ–è¯æ±‡
                        word_normalized = word.lower()
                        
                        # å¤„ç†è¯æ¡
                        word_data = self._process_dictionary_entry(
                            word=word_normalized,
                            rank=rank,
                            pos=pos
                        )
                        
                        if word_data:
                            batch_data.append(word_data)
                            
                            # æ‰¹é‡å†™å…¥æ•°æ®åº“
                            if len(batch_data) >= batch_size:
                                self._batch_insert_dictionary(batch_data)
                                stats['successfully_imported'] += len(batch_data)
                                batch_data = []
                                
                                # è¿›åº¦æŠ¥å‘Š
                                if stats['successfully_imported'] % 5000 == 0:
                                    logger.info(f"ğŸ“Š å·²å¯¼å…¥ {stats['successfully_imported']} ä¸ªè¯æ±‡...")
                    
                    except (ValueError, IndexError) as e:
                        stats['errors'] += 1
                        logger.warning(f"âŒ å¤„ç†è¡Œæ—¶å‡ºé”™: {row[:3]} - {e}")
                
                # å†™å…¥å‰©ä½™æ•°æ®
                if batch_data:
                    self._batch_insert_dictionary(batch_data)
                    stats['successfully_imported'] += len(batch_data)
        
        except FileNotFoundError:
            logger.error(f"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {coca_file_path}")
            stats['errors'] += 1
        except Exception as e:
            logger.error(f"âŒ å¯¼å…¥è¿‡ç¨‹å‡ºé”™: {e}")
            stats['errors'] += 1
        
        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        self._print_import_stats(stats)
        return stats
    
    def _process_dictionary_entry(self, word: str, rank: int, pos: str) -> Optional[Dict]:
        """å¤„ç†å•ä¸ªè¯å…¸æ¡ç›®ï¼Œè¡¥å……å®šä¹‰ç­‰ä¿¡æ¯"""
        
        # POSæ ‡å‡†åŒ–æ˜ å°„
        pos_mapping = {
            'N': 'noun', 'V': 'verb', 'A': 'adjective', 'R': 'adverb',
            'n': 'noun', 'v': 'verb', 'a': 'adjective', 'r': 'adverb',
            'ADJ': 'adjective', 'ADV': 'adverb', 'NOUN': 'noun', 'VERB': 'verb'
        }
        
        pos_standardized = pos_mapping.get(pos, pos.lower())
        
        # åŸºç¡€è¯æ¡æ•°æ®
        word_data = {
            'id': str(uuid.uuid4()),
            'word': word,
            'lemma': word,  # æš‚æ—¶è®¾ä¸ºåŒwordï¼Œåç»­å¯ä»¥ç”¨lemmatizerä¼˜åŒ–
            'pos_primary': pos_standardized,
            'frequency_rank': rank,
            'source_data': {
                'coca_pos': pos,
                'coca_rank': rank
            }
        }
        
        # è¡¥å……WordNetå®šä¹‰ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.wordnet_available:
            definition = self._get_wordnet_definition(word, pos_standardized)
            if definition:
                word_data['definition'] = definition
                word_data['source_data']['wordnet_found'] = True
        
        # è®¡ç®—éš¾åº¦ç­‰çº§ï¼ˆåŸºäºè¯é¢‘æ’åï¼‰
        word_data['difficulty_level'] = self._calculate_difficulty_level(rank)
        
        return word_data
    
    def _get_wordnet_definition(self, word: str, pos: str) -> Optional[str]:
        """ä»WordNetè·å–è¯æ±‡å®šä¹‰"""
        if not self.wordnet_available:
            return None
        
        # POSè½¬æ¢ä¸ºWordNetæ ¼å¼
        pos_map = {
            'noun': 'n',
            'verb': 'v', 
            'adjective': 'a',
            'adverb': 'r'
        }
        
        wn_pos = pos_map.get(pos)
        if not wn_pos:
            return None
        
        try:
            synsets = self.wn.synsets(word, pos=wn_pos)
            if synsets:
                # å–ç¬¬ä¸€ä¸ªåŒä¹‰è¯é›†çš„å®šä¹‰
                return synsets[0].definition()
        except Exception:
            pass
        
        return None
    
    def _calculate_difficulty_level(self, rank: int) -> int:
        """åŸºäºè¯é¢‘æ’åè®¡ç®—éš¾åº¦ç­‰çº§ (1-5)"""
        if rank <= 2000:           # æœ€å¸¸ç”¨2000è¯
            return 1
        elif rank <= 5000:         # å¸¸ç”¨5000è¯  
            return 2
        elif rank <= 15000:        # ä¸­ç­‰15000è¯
            return 3
        elif rank <= 35000:        # è¾ƒéš¾35000è¯
            return 4
        else:                      # é«˜éš¾åº¦è¯æ±‡
            return 5
    
    def _batch_insert_dictionary(self, batch_data: List[Dict]):
        """æ‰¹é‡æ’å…¥å­—å…¸æ•°æ®åˆ°æ•°æ®åº“ï¼Œæ”¯æŒå¤šè¯æ€§ç‹¬ç«‹è¯æ¡"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                conn.execute("BEGIN TRANSACTION")
                
                for word_data in batch_data:
                    word = word_data['word']
                    pos = word_data['pos_primary']
                    new_rank = word_data['frequency_rank']
                    
                    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨è¯¥è¯æ±‡+è¯æ€§ç»„åˆ
                    existing = conn.execute("""
                        SELECT frequency_rank, id FROM common_dictionary 
                        WHERE word = ? AND pos_primary = ?
                    """, (word, pos)).fetchone()
                    
                    if existing:
                        existing_rank, existing_id = existing
                        
                        # åªæœ‰æ–°è¯æ±‡çš„æ’åæ›´é«˜ï¼ˆæ•°å­—æ›´å°ï¼‰æ—¶æ‰æ›¿æ¢
                        if new_rank < existing_rank:
                            # ä¿ç•™åŸIDï¼Œæ›´æ–°å…¶ä»–å­—æ®µ
                            conn.execute("""
                                UPDATE common_dictionary 
                                SET lemma = ?, pos_primary = ?, definition = ?, frequency_rank = ?, 
                                    difficulty_level = ?, source_data = ?, created_at = ?
                                WHERE id = ?
                            """, (
                                word_data['lemma'],
                                word_data['pos_primary'],
                                word_data.get('definition'),
                                word_data['frequency_rank'],
                                word_data['difficulty_level'],
                                json.dumps(word_data['source_data']),
                                datetime.now().isoformat(),
                                existing_id
                            ))
                        # å¦‚æœæ–°è¯æ±‡æ’åæ›´ä½ï¼Œè·³è¿‡æ’å…¥ï¼ˆä¿ç•™ç°æœ‰çš„é«˜é¢‘ç‰ˆæœ¬ï¼‰
                    else:
                        # æ–°è¯æ±‡ï¼Œç›´æ¥æ’å…¥
                        conn.execute("""
                            INSERT INTO common_dictionary 
                            (id, word, lemma, pos_primary, definition, frequency_rank, 
                             difficulty_level, source_data, created_at)
                            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                        """, (
                            word_data['id'],
                            word_data['word'],
                            word_data['lemma'],
                            word_data['pos_primary'],
                            word_data.get('definition'),
                            word_data['frequency_rank'],
                            word_data['difficulty_level'],
                            json.dumps(word_data['source_data']),
                            datetime.now().isoformat()
                        ))
                
                conn.execute("COMMIT")
                
        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡æ’å…¥å¤±è´¥: {e}")
            raise
    
    def _print_import_stats(self, stats: Dict[str, int]):
        """æ‰“å°å¯¼å…¥ç»Ÿè®¡ä¿¡æ¯"""
        logger.info("=" * 50)
        logger.info("ğŸ“Š ç³»ç»Ÿè¯å…¸å¯¼å…¥å®Œæˆç»Ÿè®¡:")
        logger.info(f"   ğŸ“ æ€»å¤„ç†è¯æ±‡: {stats['total_processed']}")
        logger.info(f"   âœ… æˆåŠŸå¯¼å…¥: {stats['successfully_imported']}")
        logger.info(f"   ğŸš« è·³è¿‡ä¸“æœ‰åè¯: {stats['skipped_proper_nouns']}")
        logger.info(f"   ğŸ“– WordNetå®šä¹‰è¡¥å……: {stats['wordnet_definitions_added']}")
        logger.info(f"   âŒ å¤„ç†é”™è¯¯: {stats['errors']}")
        logger.info("=" * 50)
    
    def update_words_dictionary_mapping(self):
        """æ›´æ–°wordsè¡¨ä¸­çš„å­—å…¸æ˜ å°„å­—æ®µï¼ˆä½¿ç”¨dictionary_idï¼‰"""
        logger.info("ğŸ”„ å¼€å§‹æ›´æ–°wordsè¡¨çš„å­—å…¸æ˜ å°„...")
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                # æŸ¥è¯¢æ‰€æœ‰éœ€è¦æ›´æ–°çš„words
                cursor = conn.execute("""
                    SELECT w.id, w.lemma, w.surface_form
                    FROM words w
                    WHERE w.dictionary_found = FALSE OR w.dictionary_found IS NULL
                """)
                
                updated_count = 0
                
                for word_id, lemma, surface_form in cursor.fetchall():
                    # å…ˆå°è¯•ç²¾ç¡®åŒ¹é…lemma
                    dict_match = conn.execute("""
                        SELECT id, lemma, frequency_rank, difficulty_level
                        FROM common_dictionary
                        WHERE word = ? OR lemma = ?
                        ORDER BY frequency_rank ASC
                        LIMIT 1
                    """, (lemma, lemma)).fetchone()
                    
                    # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•åŒ¹é…surface_form
                    if not dict_match:
                        dict_match = conn.execute("""
                            SELECT id, lemma, frequency_rank, difficulty_level
                            FROM common_dictionary
                            WHERE word = ?
                            ORDER BY frequency_rank ASC
                            LIMIT 1
                        """, (surface_form,)).fetchone()
                    
                    # æ›´æ–°wordsè¡¨ï¼ˆä½¿ç”¨dictionary_idï¼‰
                    if dict_match:
                        dict_id, dict_lemma, rank, difficulty = dict_match
                        conn.execute("""
                            UPDATE words
                            SET dictionary_id = ?,
                                dictionary_found = TRUE,
                                dictionary_rank = ?,
                                difficulty_level = ?
                            WHERE id = ?
                        """, (dict_id, rank, difficulty, word_id))
                        updated_count += 1
                
                conn.commit()
                logger.info(f"âœ… æ›´æ–°äº† {updated_count} ä¸ªè¯æ±‡çš„å­—å…¸æ˜ å°„")
                
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°å­—å…¸æ˜ å°„å¤±è´¥: {e}")
            raise
    
    # =================== å­—å…¸æŸ¥è¯¢åŠŸèƒ½ ===================
    
    def query_word(self, word: str) -> List[Dict]:
        """æŸ¥è¯¢å•è¯åœ¨å­—å…¸ä¸­çš„ä¿¡æ¯ï¼Œè¿”å›æ‰€æœ‰è¯æ€§çš„ç‰ˆæœ¬"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.execute("""
                    SELECT id, word, lemma, pos_primary, definition, frequency_rank, 
                           difficulty_level, source_data
                    FROM common_dictionary
                    WHERE word = ? OR lemma = ?
                    ORDER BY frequency_rank ASC
                """, (word.lower(), word.lower()))
                
                results = []
                for row in cursor.fetchall():
                    results.append({
                        'id': row[0],
                        'word': row[1],
                        'lemma': row[2],
                        'pos_primary': row[3],
                        'definition': row[4],
                        'frequency_rank': row[5],
                        'difficulty_level': row[6],
                        'source_data': json.loads(row[7]) if row[7] else {}
                    })
                
                return results
                
        except Exception as e:
            logger.error(f"âŒ æŸ¥è¯¢è¯æ±‡å¤±è´¥: {e}")
            return []
    
    def get_dictionary_stats(self) -> Dict[str, int]:
        """è·å–å­—å…¸ç»Ÿè®¡ä¿¡æ¯"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                stats = {}
                
                # æ€»è¯æ±‡æ•°
                total = conn.execute("SELECT COUNT(*) FROM common_dictionary").fetchone()[0]
                stats['total_words'] = total
                
                # æŒ‰è¯æ€§ç»Ÿè®¡
                cursor = conn.execute("""
                    SELECT pos_primary, COUNT(*) as count
                    FROM common_dictionary
                    GROUP BY pos_primary
                    ORDER BY count DESC
                """)
                pos_stats = dict(cursor.fetchall())
                stats['pos_distribution'] = pos_stats
                
                # æŒ‰éš¾åº¦çº§åˆ«ç»Ÿè®¡
                for level in range(1, 6):
                    count = conn.execute("""
                        SELECT COUNT(*) FROM common_dictionary 
                        WHERE difficulty_level = ?
                    """, (level,)).fetchone()[0]
                    stats[f'difficulty_level_{level}'] = count
                
                # æœ‰å®šä¹‰çš„è¯æ±‡æ•°
                with_definition = conn.execute("""
                    SELECT COUNT(*) FROM common_dictionary 
                    WHERE definition IS NOT NULL
                """).fetchone()[0]
                stats['words_with_definition'] = with_definition
                
                # wordsè¡¨åŒ¹é…æƒ…å†µï¼ˆé€šè¿‡dictionary_idï¼‰
                total_user_words = conn.execute("SELECT COUNT(*) FROM words").fetchone()[0]
                matched_words = conn.execute("""
                    SELECT COUNT(*) FROM words WHERE dictionary_found = TRUE
                """).fetchone()[0]
                
                stats['total_user_words'] = total_user_words
                stats['matched_user_words'] = matched_words
                stats['match_rate'] = (matched_words / total_user_words * 100) if total_user_words > 0 else 0
                
                # ç‹¬ç‰¹è¯æ±‡å’Œå¤šè¯æ€§ç»Ÿè®¡
                unique_words = conn.execute("""
                    SELECT COUNT(DISTINCT word) FROM common_dictionary
                """).fetchone()[0]
                stats['unique_words'] = unique_words
                
                # å¤šè¯æ€§è¯æ±‡ç»Ÿè®¡
                multipos_words = conn.execute("""
                    SELECT COUNT(*) FROM (
                        SELECT word FROM common_dictionary
                        GROUP BY word
                        HAVING COUNT(*) > 1
                    )
                """).fetchone()[0]
                stats['multipos_words'] = multipos_words
                
                return stats
                
        except Exception as e:
            logger.error(f"âŒ è·å–å­—å…¸ç»Ÿè®¡å¤±è´¥: {e}")
            return {}
    
    def get_words_by_difficulty(self, difficulty_level: int, limit: int = 50) -> List[Dict]:
        """è·å–æŒ‡å®šéš¾åº¦çº§åˆ«çš„è¯æ±‡åˆ—è¡¨"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.execute("""
                    SELECT word, pos_primary, definition, frequency_rank
                    FROM common_dictionary
                    WHERE difficulty_level = ?
                    ORDER BY frequency_rank ASC
                    LIMIT ?
                """, (difficulty_level, limit))
                
                results = []
                for row in cursor.fetchall():
                    results.append({
                        'word': row[0],
                        'pos_primary': row[1],
                        'definition': row[2],
                        'frequency_rank': row[3]
                    })
                
                return results
                
        except Exception as e:
            logger.error(f"âŒ è·å–éš¾åº¦è¯æ±‡å¤±è´¥: {e}")
            return []

# ç»´æŠ¤æ€§å‘½ä»¤è¡Œå·¥å…·ï¼ˆä»…ä¾›å¼€å‘/ç®¡ç†å‘˜ä½¿ç”¨ï¼‰
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="ç³»ç»Ÿå­—å…¸ç®¡ç†å·¥å…·ï¼ˆä»…ä¾›ç»´æŠ¤ä½¿ç”¨ï¼‰")
    parser.add_argument("--import-coca", help="å¯¼å…¥COCAè¯è¡¨æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--max-words", type=int, help="æœ€å¤§å¯¼å…¥è¯æ±‡æ•°")
    parser.add_argument("--db-path", default="data/databases/unified.db", help="æ•°æ®åº“è·¯å¾„")
    parser.add_argument("--update-mapping", action="store_true", help="æ›´æ–°wordsè¡¨å­—å…¸æ˜ å°„")
    parser.add_argument("--stats", action="store_true", help="æ˜¾ç¤ºå­—å…¸ç»Ÿè®¡ä¿¡æ¯")
    parser.add_argument("--query", help="æŸ¥è¯¢å•è¯ä¿¡æ¯")
    
    args = parser.parse_args()
    
    manager = DictionaryManager(args.db_path)
    
    if args.import_coca:
        # å¯¼å…¥COCAè¯è¡¨
        stats = manager.import_coca_dictionary(
            coca_file_path=args.import_coca,
            max_words=args.max_words
        )
        
        # æ›´æ–°æ˜ å°„
        if args.update_mapping:
            manager.update_words_dictionary_mapping()
    
    elif args.stats:
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        stats = manager.get_dictionary_stats()
        print("ğŸ“– å­—å…¸ç»Ÿè®¡ä¿¡æ¯:")
        for key, value in stats.items():
            print(f"   {key}: {value}")
    
    elif args.query:
        # æŸ¥è¯¢å•è¯
        result = manager.query_word(args.query)
        if result:
            print(f"ğŸ“ è¯æ±‡ä¿¡æ¯: {args.query}")
            for key, value in result.items():
                print(f"   {key}: {value}")
        else:
            print(f"âŒ æœªæ‰¾åˆ°è¯æ±‡: {args.query}")
    
    else:
        parser.print_help() 