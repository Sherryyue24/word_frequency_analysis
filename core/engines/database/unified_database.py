# ç»Ÿä¸€æ•°æ®åº“æ“ä½œç±»
# è·¯å¾„: core/engines/unified_database.py  
# é¡¹ç›®åç§°: Word Frequency Analysis
# ä½œè€…: Sherryyue

import uuid
import sqlite3
import json
import hashlib
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
from pathlib import Path
import re

# è¯æ±‡å¤„ç†æ”¹è¿› - æ·»åŠ NLTKæ”¯æŒ
try:
    import nltk
    from nltk.stem import PorterStemmer
    
    # ç¡®ä¿å¿…è¦æ•°æ®å¯ç”¨
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        print("ğŸ“¥ é¦–æ¬¡ä½¿ç”¨ï¼Œæ­£åœ¨ä¸‹è½½NLTKæ•°æ®åŒ…...")
        nltk.download('punkt', quiet=True)
    
    # åˆ›å»ºå…¨å±€stemmerå®ä¾‹
    stemmer = PorterStemmer()
    NLTK_AVAILABLE = True
    print("âœ… NLTKè¯å¹²æå–å™¨å·²åŠ è½½")
    
except ImportError:
    stemmer = None
    NLTK_AVAILABLE = False
    print("âš ï¸  NLTKæœªå®‰è£…ï¼Œä½¿ç”¨ç®€å•è¯æ±‡æ ‡å‡†åŒ–")

from core.models.schema import ModernSchema

class UnifiedDatabase:
    """ç»Ÿä¸€çš„æ•°æ®åº“æ“ä½œç±» - å®ç°ç°ä»£åŒ–æ¶æ„"""
    
    def __init__(self, db_path: str = "data/databases/unified.db"):
        self.db_path = db_path
        Path(db_path).parent.mkdir(parents=True, exist_ok=True)
        
        # ç¡®ä¿æ•°æ®åº“æ¶æ„å­˜åœ¨
        schema = ModernSchema(db_path)
        schema.create_tables()
        schema.create_views()
    
    def _generate_uuid(self) -> str:
        """ç”ŸæˆUUIDå­—ç¬¦ä¸²"""
        return str(uuid.uuid4())
    
    def _calculate_content_hash(self, content: str) -> str:
        """è®¡ç®—å†…å®¹çš„SHA256å“ˆå¸Œ"""
        return hashlib.sha256(content.encode('utf-8')).hexdigest()
    
    # =================== æ–‡æ¡£ç®¡ç† ===================
    
    def add_document(self, filename: str, content: str, file_path: str = None, 
                    document_type: str = 'text', metadata: Dict = None) -> str:
        """æ·»åŠ æ–‡æ¡£åˆ°æ•°æ®åº“"""
        doc_id = self._generate_uuid()
        content_hash = self._calculate_content_hash(content)
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒå†…å®¹çš„æ–‡æ¡£
        existing_doc = self.get_document_by_hash(content_hash)
        if existing_doc:
            print(f"æ–‡æ¡£å·²å­˜åœ¨: {existing_doc['filename']}")
            return existing_doc['id']
        
        doc_metadata = metadata or {}
        doc_metadata.update({
            'file_size': len(content.encode('utf-8')),
            'word_count': len(content.split()) if document_type == 'text' else 0
        })
        
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                INSERT INTO documents 
                (id, filename, file_path, content_hash, file_size, status, 
                 document_type, metadata, created_at, updated_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP)
            """, (doc_id, filename, file_path, content_hash, len(content.encode('utf-8')),
                  'pending', document_type, json.dumps(doc_metadata)))
        
        print(f"âœ… æ–‡æ¡£å·²æ·»åŠ : {filename} (ID: {doc_id[:8]}...)")
        return doc_id
    
    def update_document_status(self, doc_id: str, status: str, metadata: Dict = None):
        """æ›´æ–°æ–‡æ¡£çŠ¶æ€å’Œå…ƒæ•°æ®"""
        with sqlite3.connect(self.db_path) as conn:
            if metadata:
                conn.execute("""
                    UPDATE documents 
                    SET status = ?, metadata = ?, updated_at = CURRENT_TIMESTAMP,
                        processed_at = CASE WHEN ? = 'completed' THEN CURRENT_TIMESTAMP ELSE processed_at END
                    WHERE id = ?
                """, (status, json.dumps(metadata), status, doc_id))
            else:
                conn.execute("""
                    UPDATE documents 
                    SET status = ?, updated_at = CURRENT_TIMESTAMP,
                        processed_at = CASE WHEN ? = 'completed' THEN CURRENT_TIMESTAMP ELSE processed_at END  
                    WHERE id = ?
                """, (status, status, doc_id))
    
    def get_document_by_hash(self, content_hash: str) -> Optional[Dict]:
        """æ ¹æ®å†…å®¹å“ˆå¸Œè·å–æ–‡æ¡£"""
        with sqlite3.connect(self.db_path) as conn:
            conn.row_factory = sqlite3.Row
            cursor = conn.execute("""
                SELECT * FROM documents WHERE content_hash = ?
            """, (content_hash,))
            row = cursor.fetchone()
            
            if row:
                return dict(row)
        return None
    
    def get_all_documents(self, document_type: str = None) -> List[Dict]:
        """è·å–æ‰€æœ‰æ–‡æ¡£"""
        with sqlite3.connect(self.db_path) as conn:
            conn.row_factory = sqlite3.Row
            
            if document_type:
                cursor = conn.execute("""
                    SELECT * FROM documents WHERE document_type = ? 
                    ORDER BY created_at DESC
                """, (document_type,))
            else:
                cursor = conn.execute("""
                    SELECT * FROM documents ORDER BY created_at DESC
                """)
            
            return [dict(row) for row in cursor.fetchall()]
    
    # =================== è¯æ±‡ç®¡ç† ===================
    
    def add_or_get_word(self, surface_form: str, lemma: str = None, 
                       linguistic_features: Dict = None) -> str:
        """æ·»åŠ è¯æ±‡æˆ–è·å–å·²å­˜åœ¨è¯æ±‡çš„ID - æ”¹è¿›ç‰ˆè¯æ±‡å»é‡"""
        
        # æ”¹è¿›çš„è¯æ ¹å¤„ç†é€»è¾‘
        if not lemma:
            lemma = self._get_word_lemma(surface_form)
        
        normalized_form = self._normalize_word(surface_form)
        
        with sqlite3.connect(self.db_path) as conn:
            # é¦–å…ˆå°è¯•åŸºäºlemmaæŸ¥æ‰¾ï¼ˆä¸»è¦å»é‡é€»è¾‘ï¼‰
            cursor = conn.execute("""
                SELECT id FROM words WHERE lemma = ?
            """, (lemma,))
            
            row = cursor.fetchone()
            if row:
                # æ‰¾åˆ°ç›¸åŒè¯æ ¹çš„è¯æ±‡ï¼Œæ›´æ–°surface_formï¼ˆå¦‚æœæ›´æ ‡å‡†ï¼‰
                word_id = row[0]
                self._update_word_surface_form(conn, word_id, surface_form, lemma)
                return word_id
            
            # åˆ›å»ºæ–°è¯æ±‡è®°å½•
            word_id = self._generate_uuid()
            features = json.dumps(linguistic_features) if linguistic_features else None
            
            conn.execute("""
                INSERT INTO words 
                (id, surface_form, lemma, normalized_form, linguistic_features)
                VALUES (?, ?, ?, ?, ?)
            """, (word_id, surface_form, lemma, normalized_form, features))
            
            return word_id
    
    def _get_word_lemma(self, word: str) -> str:
        """è·å–è¯æ±‡çš„è¯æ ¹å½¢å¼ - ä½¿ç”¨NLTKæ”¹è¿›"""
        word_clean = re.sub(r'[^\w]', '', word.lower())
        
        if NLTK_AVAILABLE and stemmer:
            # ä½¿ç”¨NLTK Porter Stemmer
            try:
                lemma = stemmer.stem(word_clean)
                
                # å¤„ç†ä¸€äº›å¸¸è§çš„è¿‡åº¦è¯å¹²åŒ–é—®é¢˜
                corrections = {
                    'studi': 'study',
                    'fli': 'fly', 
                    'happi': 'happy',
                    'univers': 'university'
                }
                
                lemma = corrections.get(lemma, lemma)
                return lemma
                
            except Exception as e:
                print(f"âš ï¸  NLTKå¤„ç†å¤±è´¥ {word}: {e}")
                
        # å¤‡ç”¨ï¼šç®€å•è§„åˆ™åŒ–å¤„ç†
        return self._simple_lemmatize(word_clean)
    
    def _simple_lemmatize(self, word: str) -> str:
        """ç®€å•çš„è¯æ±‡æ ‡å‡†åŒ– - å¤‡ç”¨æ–¹æ¡ˆ"""
        word = word.lower()
        
        # åŸºæœ¬å¤æ•°å¤„ç†
        if word.endswith('ies') and len(word) > 4:
            return word[:-3] + 'y'  # flies â†’ fly
        elif word.endswith('es') and len(word) > 3:
            return word[:-2]  # boxes â†’ box
        elif word.endswith('s') and len(word) > 2 and not word.endswith('ss'):
            return word[:-1]  # cats â†’ cat
        
        # åŸºæœ¬åŠ¨è¯å¤„ç†
        if word.endswith('ing') and len(word) > 4:
            base = word[:-3]
            # å¤„ç†åŒå†™å­—æ¯: running â†’ run
            if len(base) > 2 and base[-1] == base[-2] and base[-1] in 'bdfglmnprt':
                return base[:-1]
            return base
        
        elif word.endswith('ed') and len(word) > 3:
            return word[:-2]
        
        return word
    
    def _update_word_surface_form(self, conn, word_id: str, new_surface: str, lemma: str):
        """æ›´æ–°è¯æ±‡çš„è¡¨é¢å½¢å¼ï¼ˆé€‰æ‹©æ›´æ ‡å‡†çš„å½¢å¼ï¼‰"""
        try:
            # è·å–å½“å‰çš„surface_form
            cursor = conn.execute("SELECT surface_form FROM words WHERE id = ?", (word_id,))
            current_surface = cursor.fetchone()[0]
            
            # é€‰æ‹©æ›´æ ‡å‡†çš„å½¢å¼ï¼ˆä¼˜å…ˆå°å†™ã€æ›´çŸ­çš„å½¢å¼ï¼‰
            if (new_surface.lower() == lemma and 
                current_surface.lower() != lemma and 
                len(new_surface) <= len(current_surface)):
                
                conn.execute("""
                    UPDATE words SET surface_form = ? WHERE id = ?
                """, (new_surface, word_id))
                
        except Exception as e:
            # æ›´æ–°å¤±è´¥ä¸å½±å“ä¸»æµç¨‹
            pass
    
    def _normalize_word(self, word: str) -> str:
        """æ ‡å‡†åŒ–è¯æ±‡å¤„ç†"""
        # è½¬å°å†™ï¼Œç§»é™¤æ ‡ç‚¹
        normalized = re.sub(r'[^\w]', '', word.lower())
        return normalized
    
    def batch_add_words(self, words: List[str]) -> Dict[str, str]:
        """æ‰¹é‡æ·»åŠ è¯æ±‡ï¼Œè¿”å›è¯æ±‡åˆ°IDçš„æ˜ å°„"""
        word_ids = {}
        with sqlite3.connect(self.db_path) as conn:
            for word in words:
                word_id = self.add_or_get_word(word)
                word_ids[word] = word_id
        return word_ids
    
    # =================== è¯é¢‘ç®¡ç† ===================
    
    def store_word_frequencies(self, doc_id: str, word_frequencies: Dict[str, int], 
                              word_positions: Dict[str, List[int]] = None, 
                              context_text: str = None):
        """å­˜å‚¨æ–‡æ¡£çš„è¯é¢‘æ•°æ® - ç²¾ç»†åŒ–ç‰ˆæœ¬ï¼Œä¿ç•™åŸå§‹è¯æ±‡å½¢å¼"""
        if not word_frequencies:
            return
        
        # æ‰¹é‡è·å–è¯æ±‡ID - ä¸ºæ¯ä¸ªåŸå§‹è¯æ±‡å½¢å¼åˆ›å»ºç‹¬ç«‹è®°å½•
        word_ids = self.batch_add_words_detailed(list(word_frequencies.keys()), context_text)
        
        # è®¡ç®—æ€»è¯æ•°ç”¨äºTFè®¡ç®—
        total_words = sum(word_frequencies.values())
        
        with sqlite3.connect(self.db_path) as conn:
            # æ¸…é™¤å¯èƒ½å­˜åœ¨çš„æ—§æ•°æ®
            conn.execute("DELETE FROM occurrences WHERE document_id = ?", (doc_id,))
            
            # ä¸ºæ¯ä¸ªåŸå§‹è¯æ±‡å½¢å¼åˆ›å»ºç‹¬ç«‹çš„è¯é¢‘è®°å½•
            occurrence_data = []
            for word, frequency in word_frequencies.items():
                word_id = word_ids[word]
                tf_score = frequency / total_words  # è®¡ç®—TFåˆ†æ•°
                
                # å¤„ç†ä½ç½®ä¿¡æ¯
                positions = word_positions.get(word, []) if word_positions else []
                positions = sorted(list(set(positions))) if positions else []
                positions_json = json.dumps(positions) if positions else None
                first_pos = min(positions) if positions else None
                last_pos = max(positions) if positions else None
                
                occurrence_data.append((
                    doc_id, word_id, frequency, tf_score, 
                    positions_json, first_pos, last_pos
                ))
            
            conn.executemany("""
                INSERT INTO occurrences 
                (document_id, word_id, frequency, tf_score, positions, first_position, last_position)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            """, occurrence_data)
        
        print(f"âœ… å­˜å‚¨äº† {len(word_frequencies)} ä¸ªåŸå§‹è¯æ±‡çš„é¢‘ç‡æ•°æ®")
        
    def batch_add_words_detailed(self, words: List[str], context_text: str = None) -> Dict[str, str]:
        """æ‰¹é‡æ·»åŠ è¯æ±‡ï¼Œä¸ºæ¯ä¸ªåŸå§‹å½¢å¼åˆ›å»ºç‹¬ç«‹è®°å½•"""
        word_ids = {}
        with sqlite3.connect(self.db_path) as conn:
            for word in words:
                # ä¸ºæ¯ä¸ªåŸå§‹è¯æ±‡å½¢å¼åˆ›å»ºç‹¬ç«‹çš„word_idï¼Œä¼ é€’ä¸Šä¸‹æ–‡
                word_id = self.add_or_get_word_detailed(word, context_text)
                word_ids[word] = word_id
        return word_ids
    
    def add_or_get_word_detailed(self, surface_form: str, context_text: str = None) -> str:
        """æ·»åŠ æˆ–è·å–è¯æ±‡ï¼Œä¸ºæ¯ä¸ªåŸå§‹å½¢å¼åˆ›å»ºç‹¬ç«‹è®°å½•ï¼ŒåŒ…å«è¯­è¨€å­¦åˆ†æ"""
        normalized = self._normalize_word(surface_form)
        lemma = self._get_word_lemma(normalized)
        
        with sqlite3.connect(self.db_path) as conn:
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨å®Œå…¨ç›¸åŒçš„surface_form
            cursor = conn.execute("""
                SELECT id FROM words WHERE surface_form = ? AND lemma = ?
            """, (surface_form, lemma))
            
            existing = cursor.fetchone()
            if existing:
                return existing[0]
            
            # è¿›è¡Œè¯­è¨€å­¦åˆ†æ
            linguistic_features = self._analyze_linguistic_features(surface_form, context_text)
            
            # åˆ›å»ºæ–°çš„è¯æ±‡è®°å½•
            word_id = self._generate_uuid()
            conn.execute("""
                INSERT INTO words (id, surface_form, lemma, normalized_form, linguistic_features)
                VALUES (?, ?, ?, ?, ?)
            """, (word_id, surface_form, lemma, normalized, 
                  json.dumps(linguistic_features) if linguistic_features else None))
            
            return word_id
    
    def _analyze_linguistic_features(self, word: str, context_text: str = None) -> dict:
        """åˆ†æè¯æ±‡çš„è¯­è¨€å­¦ç‰¹å¾"""
        try:
            from .linguistic_analyzer import linguistic_analyzer
            
            # å¦‚æœæœ‰ä¸Šä¸‹æ–‡ï¼Œæå–ç›¸å…³è¯æ±‡ä½œä¸ºä¸Šä¸‹æ–‡
            context_words = None
            if context_text:
                # ç®€å•çš„ä¸Šä¸‹æ–‡æå– - å¯ä»¥æ ¹æ®éœ€è¦ä¼˜åŒ–
                import re
                words_in_context = re.findall(r'\b\w+\b', context_text.lower())
                # æ‰¾åˆ°ç›®æ ‡è¯æ±‡å‘¨å›´çš„è¯æ±‡
                try:
                    word_index = words_in_context.index(word.lower())
                    start = max(0, word_index - 3)
                    end = min(len(words_in_context), word_index + 4)
                    context_words = words_in_context[start:end]
                except ValueError:
                    # è¯æ±‡ä¸åœ¨ä¸Šä¸‹æ–‡ä¸­ï¼Œä½¿ç”¨æ•´ä¸ªä¸Šä¸‹æ–‡çš„å‰å‡ ä¸ªè¯
                    context_words = words_in_context[:7]
            
            features = linguistic_analyzer.analyze_word(word, context_words)
            return features
            
        except ImportError:
            print(f"âš ï¸  è¯­è¨€å­¦åˆ†æå™¨ä¸å¯ç”¨ï¼Œè·³è¿‡è¯æ±‡ {word} çš„åˆ†æ")
            return {}
        except Exception as e:
            print(f"âš ï¸  è¯­è¨€å­¦åˆ†æå¤±è´¥ {word}: {e}")
            return {}
    
    # =================== è¯­è¨€å­¦ç‰¹å¾æŸ¥è¯¢ ===================
    
    def get_word_linguistic_features(self, word: str) -> List[Dict]:
        """è·å–è¯æ±‡çš„è¯­è¨€å­¦ç‰¹å¾"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute("""
                SELECT surface_form, lemma, linguistic_features
                FROM words 
                WHERE surface_form = ? OR lemma = ?
                ORDER BY surface_form
            """, (word, word))
            
            results = []
            for surface, lemma, features_json in cursor.fetchall():
                features = json.loads(features_json) if features_json else {}
                results.append({
                    'surface_form': surface,
                    'lemma': lemma,
                    'linguistic_features': features
                })
            
            return results
    
    def get_words_by_pos_type(self, pos_type: str, limit: int = 50) -> List[Dict]:
        """æ ¹æ®è¯æ€§ç±»å‹æŸ¥è¯¢è¯æ±‡"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute("""
                SELECT w.surface_form, w.lemma, w.linguistic_features,
                       COUNT(o.frequency) as document_count,
                       SUM(o.frequency) as total_frequency
                FROM words w
                LEFT JOIN occurrences o ON w.id = o.word_id
                WHERE w.linguistic_features IS NOT NULL
                  AND JSON_EXTRACT(w.linguistic_features, '$.pos_type') = ?
                GROUP BY w.id
                ORDER BY total_frequency DESC
                LIMIT ?
            """, (pos_type, limit))
            
            results = []
            for surface, lemma, features_json, doc_count, total_freq in cursor.fetchall():
                features = json.loads(features_json) if features_json else {}
                results.append({
                    'surface_form': surface,
                    'lemma': lemma,
                    'linguistic_features': features,
                    'document_count': doc_count or 0,
                    'total_frequency': total_freq or 0
                })
            
            return results
    
    def get_pos_distribution(self) -> Dict:
        """è·å–è¯æ€§åˆ†å¸ƒç»Ÿè®¡"""
        with sqlite3.connect(self.db_path) as conn:
            # è¯æ€§ç±»å‹åˆ†å¸ƒ
            cursor = conn.execute("""
                SELECT JSON_EXTRACT(linguistic_features, '$.pos_type') as pos_type,
                       COUNT(*) as count
                FROM words 
                WHERE linguistic_features IS NOT NULL
                GROUP BY pos_type
                ORDER BY count DESC
            """)
            
            pos_type_distribution = dict(cursor.fetchall())
            
            # è¯¦ç»†è¯æ€§æ ‡ç­¾åˆ†å¸ƒ
            cursor = conn.execute("""
                SELECT JSON_EXTRACT(linguistic_features, '$.pos_tag') as pos_tag,
                       JSON_EXTRACT(linguistic_features, '$.pos_description') as description,
                       COUNT(*) as count
                FROM words 
                WHERE linguistic_features IS NOT NULL
                GROUP BY pos_tag
                ORDER BY count DESC
                LIMIT 20
            """)
            
            pos_tag_distribution = []
            for pos_tag, description, count in cursor.fetchall():
                pos_tag_distribution.append({
                    'pos_tag': pos_tag,
                    'description': description,
                    'count': count
                })
            
            # å½¢æ€å­¦å¤æ‚åº¦åˆ†å¸ƒ
            cursor = conn.execute("""
                SELECT JSON_EXTRACT(linguistic_features, '$.morphology.complexity') as complexity,
                       COUNT(*) as count
                FROM words 
                WHERE linguistic_features IS NOT NULL
                  AND JSON_EXTRACT(linguistic_features, '$.morphology') IS NOT NULL
                GROUP BY complexity
                ORDER BY count DESC
            """)
            
            morphology_distribution = dict(cursor.fetchall())
            
            return {
                'pos_type_distribution': pos_type_distribution,
                'pos_tag_distribution': pos_tag_distribution,
                'morphology_distribution': morphology_distribution,
                'total_analyzed_words': sum(pos_type_distribution.values())
            }
    
    def get_complex_words_analysis(self) -> Dict:
        """åˆ†æè¯æ±‡çš„å¤æ‚åº¦"""
        with sqlite3.connect(self.db_path) as conn:
            # æœ‰å‰ç¼€çš„è¯æ±‡
            cursor = conn.execute("""
                SELECT w.surface_form, w.lemma,
                       JSON_EXTRACT(w.linguistic_features, '$.morphology.prefix') as prefix,
                       SUM(o.frequency) as total_frequency
                FROM words w
                LEFT JOIN occurrences o ON w.id = o.word_id
                WHERE w.linguistic_features IS NOT NULL
                  AND JSON_EXTRACT(w.linguistic_features, '$.morphology.prefix') IS NOT NULL
                GROUP BY w.id
                ORDER BY total_frequency DESC
                LIMIT 20
            """)
            
            prefixed_words = []
            for surface, lemma, prefix, freq in cursor.fetchall():
                prefixed_words.append({
                    'surface_form': surface,
                    'lemma': lemma,
                    'prefix': prefix,
                    'total_frequency': freq or 0
                })
            
            # æœ‰åç¼€çš„è¯æ±‡
            cursor = conn.execute("""
                SELECT w.surface_form, w.lemma,
                       JSON_EXTRACT(w.linguistic_features, '$.morphology.suffix') as suffix,
                       JSON_EXTRACT(w.linguistic_features, '$.morphology.suffix_meaning') as suffix_meaning,
                       SUM(o.frequency) as total_frequency
                FROM words w
                LEFT JOIN occurrences o ON w.id = o.word_id
                WHERE w.linguistic_features IS NOT NULL
                  AND JSON_EXTRACT(w.linguistic_features, '$.morphology.suffix') IS NOT NULL
                GROUP BY w.id
                ORDER BY total_frequency DESC
                LIMIT 20
            """)
            
            suffixed_words = []
            for surface, lemma, suffix, suffix_meaning, freq in cursor.fetchall():
                suffixed_words.append({
                    'surface_form': surface,
                    'lemma': lemma,
                    'suffix': suffix,
                    'suffix_meaning': suffix_meaning,
                    'total_frequency': freq or 0
                })
            
            return {
                'prefixed_words': prefixed_words,
                'suffixed_words': suffixed_words
            }

    # =================== ç²¾ç»†åŒ–è¯æ±‡æŸ¥è¯¢ ===================
    
    def get_word_variants_with_frequencies(self, word: str, doc_id: str = None) -> Dict:
        """è·å–æŒ‡å®šè¯æ±‡çš„æ‰€æœ‰å˜å½¢åŠå…¶é¢‘ç‡ä¿¡æ¯"""
        lemma = self._get_word_lemma(word)
        
        with sqlite3.connect(self.db_path) as conn:
            if doc_id:
                # æŸ¥è¯¢ç‰¹å®šæ–‡æ¡£ä¸­çš„è¯æ±‡å˜å½¢
                cursor = conn.execute("""
                    SELECT w.surface_form, w.lemma, o.frequency, o.tf_score, d.filename
                    FROM words w
                    JOIN occurrences o ON w.id = o.word_id
                    JOIN documents d ON o.document_id = d.id
                    WHERE w.lemma = ? AND o.document_id = ?
                    ORDER BY o.frequency DESC
                """, (lemma, doc_id))
            else:
                # æŸ¥è¯¢æ‰€æœ‰æ–‡æ¡£ä¸­çš„è¯æ±‡å˜å½¢
                cursor = conn.execute("""
                    SELECT w.surface_form, w.lemma, 
                           SUM(o.frequency) as total_frequency,
                           COUNT(DISTINCT o.document_id) as document_count,
                           AVG(o.frequency) as avg_frequency
                    FROM words w
                    JOIN occurrences o ON w.id = o.word_id
                    WHERE w.lemma = ?
                    GROUP BY w.surface_form
                    ORDER BY total_frequency DESC
                """, (lemma,))
            
            variants = [dict(zip([col[0] for col in cursor.description], row)) 
                       for row in cursor.fetchall()]
            
            return {
                'search_word': word,
                'lemma': lemma,
                'variants': variants,
                'total_variants': len(variants),
                'total_frequency': sum(v.get('total_frequency', v.get('frequency', 0)) for v in variants)
            }
    
    def get_unique_lemma_count(self) -> Dict:
        """è·å–ç‹¬ç‰¹è¯æ ¹æ•°é‡ç»Ÿè®¡"""
        with sqlite3.connect(self.db_path) as conn:
            # ç»Ÿè®¡ç‹¬ç‰¹è¯æ ¹æ•°é‡
            cursor = conn.execute("""
                SELECT COUNT(DISTINCT lemma) as unique_lemmas
                FROM words
            """)
            unique_lemmas = cursor.fetchone()[0]
            
            # ç»Ÿè®¡åŸå§‹è¯æ±‡å½¢å¼æ•°é‡
            cursor = conn.execute("""
                SELECT COUNT(*) as total_surface_forms
                FROM words
            """)
            total_surface_forms = cursor.fetchone()[0]
            
            # ç»Ÿè®¡å¹³å‡æ¯ä¸ªè¯æ ¹æœ‰å¤šå°‘å˜å½¢
            cursor = conn.execute("""
                SELECT lemma, COUNT(*) as variant_count
                FROM words
                GROUP BY lemma
                ORDER BY variant_count DESC
                LIMIT 10
            """)
            top_varied_lemmas = cursor.fetchall()
            
            return {
                'unique_lemmas': unique_lemmas,
                'total_surface_forms': total_surface_forms,
                'avg_variants_per_lemma': total_surface_forms / unique_lemmas if unique_lemmas > 0 else 0,
                'most_varied_lemmas': [{'lemma': lemma, 'variant_count': count} 
                                     for lemma, count in top_varied_lemmas]
            }
    
    def get_lemma_analysis(self, doc_id: str = None) -> Dict:
        """è·å–è¯æ ¹çº§åˆ«çš„åˆ†æï¼Œæ”¯æŒæŒ‰æ–‡æ¡£ç­›é€‰"""
        with sqlite3.connect(self.db_path) as conn:
            if doc_id:
                # ç‰¹å®šæ–‡æ¡£çš„è¯æ ¹åˆ†æ
                cursor = conn.execute("""
                    SELECT 
                        w.lemma,
                        COUNT(DISTINCT w.surface_form) as variant_count,
                        SUM(o.frequency) as total_frequency,
                        GROUP_CONCAT(w.surface_form || ':' || o.frequency) as variants_detail
                    FROM words w
                    JOIN occurrences o ON w.id = o.word_id
                    WHERE o.document_id = ?
                    GROUP BY w.lemma
                    ORDER BY total_frequency DESC
                """, (doc_id,))
            else:
                # å…¨å±€è¯æ ¹åˆ†æ
                cursor = conn.execute("""
                    SELECT 
                        w.lemma,
                        COUNT(DISTINCT w.surface_form) as variant_count,
                        SUM(o.frequency) as total_frequency,
                        COUNT(DISTINCT o.document_id) as document_count
                    FROM words w
                    JOIN occurrences o ON w.id = o.word_id
                    GROUP BY w.lemma
                    ORDER BY total_frequency DESC
                """)
            
            lemmas = [dict(zip([col[0] for col in cursor.description], row)) 
                     for row in cursor.fetchall()]
            
            return {
                'document_id': doc_id,
                'total_lemmas': len(lemmas),
                'lemma_analysis': lemmas
            }

    # =================== è¯æ±‡è¡¨ç®¡ç† ===================
    
    def create_wordlist(self, name: str, description: str = None, 
                       source_file: str = None, metadata: Dict = None) -> str:
        """åˆ›å»ºè¯æ±‡è¡¨"""
        wordlist_id = self._generate_uuid()
        
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                INSERT INTO wordlists (id, name, description, source_file, metadata)
                VALUES (?, ?, ?, ?, ?)
            """, (wordlist_id, name, description, source_file, 
                  json.dumps(metadata) if metadata else None))
        
        print(f"âœ… åˆ›å»ºè¯æ±‡è¡¨: {name} (ID: {wordlist_id[:8]}...)")
        return wordlist_id
    
    def add_words_to_wordlist(self, wordlist_id: str, words: List[str], 
                            confidence: float = 1.0) -> Dict[str, int]:
        """å°†è¯æ±‡æ‰¹é‡æ·»åŠ åˆ°è¯æ±‡è¡¨ï¼Œè¿”å›è¯¦ç»†ç»Ÿè®¡"""
        word_ids = self.batch_add_words(words)
        
        with sqlite3.connect(self.db_path) as conn:
            # æ£€æŸ¥ç°æœ‰å…³è”
            existing_memberships = set()
            if word_ids:
                placeholders = ','.join(['?' for _ in word_ids.values()])
                cursor = conn.execute(f"""
                    SELECT word_id FROM word_wordlist_memberships 
                    WHERE wordlist_id = ? AND word_id IN ({placeholders})
                """, [wordlist_id] + list(word_ids.values()))
                existing_memberships = {row[0] for row in cursor.fetchall()}
            
            # å‡†å¤‡æ’å…¥æ•°æ® - å»é‡word_ids
            unique_word_ids = list(set(word_ids.values()))
            membership_data = []
            new_associations = 0
            existing_associations = 0
            
            for word_id in unique_word_ids:
                if word_id in existing_memberships:
                    existing_associations += 1
                else:
                    membership_data.append((word_id, wordlist_id, confidence))
                    new_associations += 1
            
            # æ’å…¥æ–°å…³è”
            if membership_data:
                conn.executemany("""
                    INSERT INTO word_wordlist_memberships 
                    (word_id, wordlist_id, confidence)
                    VALUES (?, ?, ?)
                """, membership_data)
            
            # æ›´æ–°è¯æ±‡è¡¨çš„è¯æ±‡æ•°é‡
            conn.execute("""
                UPDATE wordlists 
                SET word_count = (
                    SELECT COUNT(*) FROM word_wordlist_memberships 
                    WHERE wordlist_id = ?
                )
                WHERE id = ?
            """, (wordlist_id, wordlist_id))
        
        print(f"âœ… æ·»åŠ äº† {new_associations} ä¸ªè¯æ±‡åˆ°è¯æ±‡è¡¨")
        if existing_associations > 0:
            print(f"ğŸ“‹ è·³è¿‡äº† {existing_associations} ä¸ªå·²å­˜åœ¨çš„è¯æ±‡")
        
        # è®¡ç®—åŸºäºåŸå§‹è¯æ±‡çš„ç»Ÿè®¡
        total_input_words = len(words)
        unique_word_count = len(unique_word_ids)
        
        # ç”±äºè¯æ±‡å»é‡åˆå¹¶ï¼Œéœ€è¦è°ƒæ•´ç»Ÿè®¡æ–¹å¼
        # å¦‚æœæœ‰è¯æ±‡åˆå¹¶ï¼Œexisting_associationsåº”è¯¥æŒ‰æ¯”ä¾‹è°ƒæ•´
        if unique_word_count < total_input_words:
            # æœ‰è¯æ±‡è¢«åˆå¹¶äº†ï¼ŒæŒ‰æ¯”ä¾‹åˆ†é…
            ratio = total_input_words / unique_word_count if unique_word_count > 0 else 1
            adjusted_new = int(new_associations * ratio)
            adjusted_existing = total_input_words - adjusted_new
        else:
            adjusted_new = new_associations
            adjusted_existing = existing_associations
        
        return {
            'total_words': total_input_words,
            'new_associations': adjusted_new,
            'existing_associations': adjusted_existing,
            'unique_word_ids': unique_word_count,
            'success': True
        }
    
    def get_wordlist_by_name(self, name: str) -> Optional[Dict]:
        """æ ¹æ®åç§°è·å–è¯æ±‡è¡¨"""
        with sqlite3.connect(self.db_path) as conn:
            conn.row_factory = sqlite3.Row
            cursor = conn.execute("""
                SELECT * FROM wordlists WHERE name = ?
            """, (name,))
            row = cursor.fetchone()
            
            if row:
                return dict(row)
        return None
    
    # =================== åˆ†ææŸ¥è¯¢ ===================
    
    def get_vocabulary_coverage(self, doc_id: str) -> List[Dict]:
        """è·å–æ–‡æ¡£çš„è¯æ±‡è¦†ç›–åº¦åˆ†æ"""
        with sqlite3.connect(self.db_path) as conn:
            conn.row_factory = sqlite3.Row
            cursor = conn.execute("""
                SELECT * FROM document_vocabulary_coverage 
                WHERE document_id = ?
                ORDER BY covered_words DESC
            """, (doc_id,))
            
            return [dict(row) for row in cursor.fetchall()]
    
    def get_word_usage_stats(self, min_frequency: int = 1) -> List[Dict]:
        """è·å–è¯æ±‡ä½¿ç”¨ç»Ÿè®¡"""
        with sqlite3.connect(self.db_path) as conn:
            conn.row_factory = sqlite3.Row
            cursor = conn.execute("""
                SELECT * FROM word_usage_stats 
                WHERE total_frequency >= ?
                ORDER BY total_frequency DESC
            """, (min_frequency,))
            
            return [dict(row) for row in cursor.fetchall()]
    
    def analyze_document_similarity(self, doc_id1: str, doc_id2: str) -> Dict:
        """åˆ†æä¸¤ä¸ªæ–‡æ¡£çš„ç›¸ä¼¼åº¦"""
        # è·å–ä¸¤ä¸ªæ–‡æ¡£çš„è¯æ±‡é›†åˆ
        with sqlite3.connect(self.db_path) as conn:
            # æ–‡æ¡£1çš„è¯æ±‡
            cursor1 = conn.execute("""
                SELECT w.lemma, o.tf_score
                FROM occurrences o
                JOIN words w ON o.word_id = w.id
                WHERE o.document_id = ?
            """, (doc_id1,))
            doc1_words = {row[0]: row[1] for row in cursor1.fetchall()}
            
            # æ–‡æ¡£2çš„è¯æ±‡  
            cursor2 = conn.execute("""
                SELECT w.lemma, o.tf_score
                FROM occurrences o
                JOIN words w ON o.word_id = w.id
                WHERE o.document_id = ?
            """, (doc_id2,))
            doc2_words = {row[0]: row[1] for row in cursor2.fetchall()}
        
        # è®¡ç®—Jaccardç›¸ä¼¼åº¦
        common_words = set(doc1_words.keys()) & set(doc2_words.keys())
        all_words = set(doc1_words.keys()) | set(doc2_words.keys())
        
        jaccard_similarity = len(common_words) / len(all_words) if all_words else 0
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        cosine_similarity = self._calculate_cosine_similarity(doc1_words, doc2_words)
        
        return {
            'document1_id': doc_id1,
            'document2_id': doc_id2,
            'jaccard_similarity': jaccard_similarity,
            'cosine_similarity': cosine_similarity,
            'common_words_count': len(common_words),
            'total_unique_words': len(all_words)
        }
    
    def _calculate_cosine_similarity(self, doc1_words: Dict, doc2_words: Dict) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        import math
        
        # è·å–æ‰€æœ‰è¯æ±‡
        all_words = set(doc1_words.keys()) | set(doc2_words.keys())
        
        # æ„å»ºå‘é‡
        vec1 = [doc1_words.get(word, 0) for word in all_words]
        vec2 = [doc2_words.get(word, 0) for word in all_words]
        
        # è®¡ç®—ç‚¹ç§¯
        dot_product = sum(a * b for a, b in zip(vec1, vec2))
        
        # è®¡ç®—å‘é‡é•¿åº¦
        norm1 = math.sqrt(sum(a * a for a in vec1))
        norm2 = math.sqrt(sum(b * b for b in vec2))
        
        # é¿å…é™¤é›¶
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return dot_product / (norm1 * norm2)
    
    # =================== å·¥å…·æ–¹æ³• ===================
    
    def get_database_stats(self) -> Dict:
        """è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
        with sqlite3.connect(self.db_path) as conn:
            stats = {}
            
            # åŸºæœ¬è®¡æ•°
            tables = ['documents', 'words', 'wordlists', 'occurrences', 'word_wordlist_memberships']
            for table in tables:
                cursor = conn.execute(f"SELECT COUNT(*) FROM {table}")
                stats[f'{table}_count'] = cursor.fetchone()[0]
            
            # æŒ‰çŠ¶æ€ç»Ÿè®¡æ–‡æ¡£
            cursor = conn.execute("""
                SELECT status, COUNT(*) FROM documents GROUP BY status
            """)
            stats['documents_by_status'] = dict(cursor.fetchall())
            
            # æŒ‰ç±»å‹ç»Ÿè®¡æ–‡æ¡£
            cursor = conn.execute("""
                SELECT document_type, COUNT(*) FROM documents GROUP BY document_type  
            """)
            stats['documents_by_type'] = dict(cursor.fetchall())
            
            return stats
    
    def cleanup_expired_cache(self):
        """æ¸…ç†è¿‡æœŸçš„åˆ†æç»“æœç¼“å­˜"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute("""
                DELETE FROM analysis_results 
                WHERE expires_at < CURRENT_TIMESTAMP
            """)
            deleted_count = cursor.rowcount
            
        if deleted_count > 0:
            print(f"ğŸ§¹ æ¸…ç†äº† {deleted_count} ä¸ªè¿‡æœŸç¼“å­˜")
        
        return deleted_count
    
    def delete_document(self, doc_id: str) -> bool:
        """åˆ é™¤å•ä¸ªæ–‡æ¡£åŠå…¶ç›¸å…³æ•°æ®"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                # å¤–é”®çº¦æŸä¼šè‡ªåŠ¨çº§è”åˆ é™¤ç›¸å…³è®°å½•
                cursor = conn.execute("DELETE FROM documents WHERE id = ?", (doc_id,))
                deleted = cursor.rowcount > 0
                
                if deleted:
                    print(f"âœ… å·²åˆ é™¤æ–‡æ¡£: {doc_id[:8]}...")
                
                return deleted
        except Exception as e:
            print(f"âŒ åˆ é™¤æ–‡æ¡£å¤±è´¥ {doc_id}: {e}")
            return False
    
    def delete_documents_by_type(self, document_type: str) -> int:
        """æŒ‰ç±»å‹æ‰¹é‡åˆ é™¤æ–‡æ¡£"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.execute("""
                    DELETE FROM documents WHERE document_type = ?
                """, (document_type,))
                deleted_count = cursor.rowcount
                
                if deleted_count > 0:
                    print(f"âœ… åˆ é™¤äº† {deleted_count} ä¸ª {document_type} ç±»å‹çš„æ–‡æ¡£")
                
                return deleted_count
        except Exception as e:
            print(f"âŒ æ‰¹é‡åˆ é™¤å¤±è´¥: {e}")
            return 0 