# æ•°æ®åº“æ¶æ„è®¾è®¡
# è·¯å¾„: core/models/schema.py
# é¡¹ç›®åç§°: Word Frequency Analysis  
# ä½œè€…: Sherryyue

import uuid
import sqlite3
from datetime import datetime
from typing import Dict, List, Optional
from pathlib import Path
import json

class ModernSchema:
    """ç°ä»£åŒ–çš„æ•°æ®åº“æ¶æ„è®¾è®¡"""
    
    def __init__(self, db_path: str = "data/databases/unified.db"):
        self.db_path = db_path
        Path(db_path).parent.mkdir(parents=True, exist_ok=True)
    
    def create_tables(self):
        """åˆ›å»ºä¼˜åŒ–åçš„è¡¨ç»“æ„"""
        with sqlite3.connect(self.db_path) as conn:
            # å¯ç”¨å¤–é”®çº¦æŸ
            conn.execute("PRAGMA foreign_keys = ON")
            
            # 1. æ–‡æ¡£è¡¨ - ç»Ÿä¸€æ–‡ä»¶ç®¡ç†
            conn.execute("""
                CREATE TABLE IF NOT EXISTS documents (
                    id TEXT PRIMARY KEY,                    -- UUID
                    filename TEXT NOT NULL,
                    file_path TEXT,
                    content_hash TEXT UNIQUE NOT NULL,      -- SHA256å†…å®¹å“ˆå¸Œ
                    file_size INTEGER,
                    status TEXT DEFAULT 'pending',          -- pending/processing/completed/failed
                    document_type TEXT DEFAULT 'text',      -- text/vocabulary_list
                    metadata JSON,                          -- çµæ´»çš„å…ƒæ•°æ®å­˜å‚¨
                    processed_at TIMESTAMP,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            # 2. è¯æ±‡è¡¨ - ç»Ÿä¸€è¯æ±‡å¤„ç†æ ¸å¿ƒ
            conn.execute("""
                CREATE TABLE IF NOT EXISTS words (
                    id TEXT PRIMARY KEY,                    -- UUID
                    surface_form TEXT NOT NULL,             -- åŸå§‹å½¢å¼ "running"
                    lemma TEXT NOT NULL,                    -- è¯æ ¹å½¢å¼ "run" 
                    stem TEXT,                              -- è¯å¹²å½¢å¼
                    normalized_form TEXT,                   -- æ ‡å‡†åŒ–å½¢å¼
                    idf_score REAL DEFAULT 0.0,            -- é€†æ–‡æ¡£é¢‘ç‡
                    linguistic_features JSON,               -- è¯æ€§ã€è¯­ä¹‰ç‰¹å¾ç­‰
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE(surface_form, lemma)
                )
            """)
            
            # 3. è¯æ±‡åˆ—è¡¨/æ ‡ç­¾ - è¯æ±‡è¡¨ç®¡ç† 
            conn.execute("""
                CREATE TABLE IF NOT EXISTS wordlists (
                    id TEXT PRIMARY KEY,                    -- UUID
                    name TEXT UNIQUE NOT NULL,              -- IELTS/GRE/TOEFLç­‰
                    version TEXT DEFAULT '1.0',
                    description TEXT,
                    source_file TEXT,                       -- æ¥æºæ–‡ä»¶
                    word_count INTEGER DEFAULT 0,
                    metadata JSON,                          -- é¢å¤–ä¿¡æ¯
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            # 4. æ–‡æ¡£-è¯æ±‡å…³è” - æ ¸å¿ƒé¢‘ç‡è¡¨
            conn.execute("""
                CREATE TABLE IF NOT EXISTS occurrences (
                    document_id TEXT,
                    word_id TEXT,
                    frequency INTEGER NOT NULL DEFAULT 1,
                    tf_score REAL DEFAULT 0.0,             -- è¯é¢‘å¾—åˆ†
                    positions JSON,                         -- è¯æ±‡åœ¨æ–‡æ¡£ä¸­çš„ä½ç½® [12, 45, 78]
                    first_position INTEGER,                 -- é¦–æ¬¡å‡ºç°ä½ç½®
                    last_position INTEGER,                  -- æœ€åå‡ºç°ä½ç½®
                    indexed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    PRIMARY KEY (document_id, word_id),
                    FOREIGN KEY (document_id) REFERENCES documents(id) ON DELETE CASCADE,
                    FOREIGN KEY (word_id) REFERENCES words(id) ON DELETE CASCADE
                )
            """)
            
            # 5. è¯æ±‡-è¯æ±‡è¡¨å…³è”
            conn.execute("""
                CREATE TABLE IF NOT EXISTS word_wordlist_memberships (
                    word_id TEXT,
                    wordlist_id TEXT,
                    confidence REAL DEFAULT 1.0,           -- ç½®ä¿¡åº¦(æœªæ¥ç”¨äºæ¨¡ç³ŠåŒ¹é…)
                    source_metadata JSON,                   -- æ¥æºä¿¡æ¯
                    added_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    PRIMARY KEY (word_id, wordlist_id),
                    FOREIGN KEY (word_id) REFERENCES words(id) ON DELETE CASCADE,
                    FOREIGN KEY (wordlist_id) REFERENCES wordlists(id) ON DELETE CASCADE  
                )
            """)
            
            # 6. åˆ†æç»“æœç¼“å­˜
            conn.execute("""
                CREATE TABLE IF NOT EXISTS analysis_results (
                    id TEXT PRIMARY KEY,                    -- UUID
                    document_id TEXT,
                    analysis_type TEXT,                     -- vocabulary_coverage/similarity/etc
                    metrics JSON,                           -- åˆ†ææŒ‡æ ‡
                    vocabulary_coverage JSON,               -- è¯æ±‡è¦†ç›–åº¦è¯¦æƒ…
                    computed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    expires_at TIMESTAMP,                   -- ç¼“å­˜è¿‡æœŸæ—¶é—´
                    FOREIGN KEY (document_id) REFERENCES documents(id) ON DELETE CASCADE
                )
            """)
            
            # 7. åˆ›å»ºç´¢å¼•æå‡æŸ¥è¯¢æ€§èƒ½
            self._create_indexes(conn)
            
            conn.commit()
    
    def _create_indexes(self, conn):
        """åˆ›å»ºä¼˜åŒ–æŸ¥è¯¢çš„ç´¢å¼•"""
        indexes = [
            "CREATE INDEX IF NOT EXISTS idx_documents_content_hash ON documents(content_hash)",
            "CREATE INDEX IF NOT EXISTS idx_documents_status ON documents(status)",
            "CREATE INDEX IF NOT EXISTS idx_documents_type ON documents(document_type)",
            
            "CREATE INDEX IF NOT EXISTS idx_words_surface_form ON words(surface_form)",
            "CREATE INDEX IF NOT EXISTS idx_words_lemma ON words(lemma)",
            "CREATE INDEX IF NOT EXISTS idx_words_idf_score ON words(idf_score)",
            
            "CREATE INDEX IF NOT EXISTS idx_wordlists_name ON wordlists(name)",
            
            "CREATE INDEX IF NOT EXISTS idx_occurrences_frequency ON occurrences(frequency)",
            "CREATE INDEX IF NOT EXISTS idx_occurrences_tf_score ON occurrences(tf_score)",
            
            "CREATE INDEX IF NOT EXISTS idx_memberships_wordlist ON word_wordlist_memberships(wordlist_id)",
            
            "CREATE INDEX IF NOT EXISTS idx_analysis_type ON analysis_results(analysis_type)",
            "CREATE INDEX IF NOT EXISTS idx_analysis_expires ON analysis_results(expires_at)"
        ]
        
        for index_sql in indexes:
            conn.execute(index_sql)

    def create_views(self):
        """åˆ›å»ºä¾¿äºæŸ¥è¯¢çš„è§†å›¾"""
        with sqlite3.connect(self.db_path) as conn:
            # æ–‡æ¡£è¯æ±‡è¦†ç›–åº¦è§†å›¾
            conn.execute("""
                CREATE VIEW IF NOT EXISTS document_vocabulary_coverage AS
                SELECT 
                    d.id as document_id,
                    d.filename,
                    wl.name as wordlist_name,
                    COUNT(DISTINCT w.id) as covered_words,
                    SUM(o.frequency) as total_frequency,
                    AVG(o.tf_score) as avg_tf_score
                FROM documents d
                JOIN occurrences o ON d.id = o.document_id
                JOIN words w ON o.word_id = w.id  
                JOIN word_wordlist_memberships m ON w.id = m.word_id
                JOIN wordlists wl ON m.wordlist_id = wl.id
                GROUP BY d.id, wl.id
            """)
            
            # è¯æ±‡ä½¿ç”¨ç»Ÿè®¡è§†å›¾
            conn.execute("""
                CREATE VIEW IF NOT EXISTS word_usage_stats AS
                SELECT 
                    w.id as word_id,
                    w.surface_form,
                    w.lemma,
                    COUNT(DISTINCT o.document_id) as document_count,
                    SUM(o.frequency) as total_frequency,
                    AVG(o.frequency) as avg_frequency,
                    MAX(o.frequency) as max_frequency,
                    w.idf_score
                FROM words w
                LEFT JOIN occurrences o ON w.id = o.word_id
                GROUP BY w.id
            """)
            
            conn.commit()

class DatabaseMigration:
    """æ•°æ®åº“è¿ç§»å·¥å…·"""
    
    def __init__(self, old_db_paths: Dict[str, str], new_db_path: str):
        self.old_analysis_db = old_db_paths.get('analysis', 'data/databases/analysis.db')
        self.old_vocabulary_db = old_db_paths.get('vocabulary', 'data/databases/vocabulary.db')  
        self.new_db_path = new_db_path
        
    def migrate(self):
        """æ‰§è¡Œå®Œæ•´è¿ç§»"""
        print("ğŸ”„ å¼€å§‹æ•°æ®åº“è¿ç§»...")
        
        # 1. åˆ›å»ºæ–°æ¶æ„
        schema = ModernSchema(self.new_db_path)
        schema.create_tables()
        schema.create_views()
        print("âœ… æ–°æ¶æ„åˆ›å»ºå®Œæˆ")
        
        # 2. è¿ç§»æ•°æ®
        self._migrate_documents()
        self._migrate_words_and_frequencies() 
        self._migrate_wordlists()
        self._update_statistics()
        
        print("ğŸ‰ æ•°æ®åº“è¿ç§»å®Œæˆï¼")
    
    def _migrate_documents(self):
        """è¿ç§»æ–‡æ¡£æ•°æ®"""
        with sqlite3.connect(self.old_analysis_db) as old_conn, \
             sqlite3.connect(self.new_db_path) as new_conn:
            
            cursor = old_conn.execute("""
                SELECT filename, content_hash, total_words, unique_words, 
                       metadata, process_duration, created_at
                FROM text_analysis
            """)
            
            for row in cursor.fetchall():
                filename, content_hash, total_words, unique_words, metadata, duration, created_at = row
                
                doc_id = str(uuid.uuid4())
                new_metadata = {
                    'total_words': total_words,
                    'unique_words': unique_words, 
                    'process_duration': duration,
                    'legacy_metadata': json.loads(metadata) if metadata else {}
                }
                
                new_conn.execute("""
                    INSERT INTO documents 
                    (id, filename, content_hash, status, metadata, processed_at, created_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                """, (doc_id, filename, content_hash, 'completed', 
                      json.dumps(new_metadata), created_at, created_at))
        
        print("âœ… æ–‡æ¡£æ•°æ®è¿ç§»å®Œæˆ")
    
    def _migrate_words_and_frequencies(self):
        """è¿ç§»è¯æ±‡å’Œé¢‘ç‡æ•°æ®"""
        # å®ç°è¯æ±‡æ•°æ®è¿ç§»é€»è¾‘
        print("âœ… è¯æ±‡æ•°æ®è¿ç§»å®Œæˆ")
    
    def _migrate_wordlists(self):
        """è¿ç§»è¯æ±‡è¡¨æ•°æ®"""
        # å®ç°è¯æ±‡è¡¨è¿ç§»é€»è¾‘
        print("âœ… è¯æ±‡è¡¨æ•°æ®è¿ç§»å®Œæˆ")
    
    def _update_statistics(self):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        # è®¡ç®—IDFåˆ†æ•°ç­‰ç»Ÿè®¡æŒ‡æ ‡
        print("âœ… ç»Ÿè®¡ä¿¡æ¯æ›´æ–°å®Œæˆ")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºæ–°æ¶æ„
    schema = ModernSchema()
    schema.create_tables()
    schema.create_views()
    
    # æ‰§è¡Œè¿ç§»
    migration = DatabaseMigration(
        old_db_paths={
            'analysis': 'data/databases/analysis.db',
            'vocabulary': 'data/databases/vocabulary.db'
        },
        new_db_path='data/databases/unified.db'
    )
    migration.migrate() 